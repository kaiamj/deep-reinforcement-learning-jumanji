{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/instadeepai/jumanji.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcadaf",
   "metadata": {},
   "source": [
    "## to run the binBox visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jumanji\n",
    "from jumanji.wrappers import AutoResetWrapper\n",
    "key = jax.random.PRNGKey(0)\n",
    "env = jumanji.make(\"BinPack-toy-v0\")\n",
    "state, timestep = jax.jit(env.reset)(key)\n",
    "\n",
    "# Randomly choose ems_id and item_id using the action mask\n",
    "for i in range(0,20):\n",
    "    num_ems, num_items = env.action_spec().num_values\n",
    "    ems_item_id = jax.random.choice(\n",
    "        key=key,\n",
    "        a=num_ems * num_items,\n",
    "        p=timestep.observation.action_mask.flatten(),\n",
    "    )\n",
    "    ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
    "\n",
    "    # Wrap the action as a jax array of shape (2,)\n",
    "    action = jnp.array([ems_id, item_id])\n",
    "    state, timestep = env.step(state, action)\n",
    "    env.render(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c6770",
   "metadata": {},
   "source": [
    "##  timestep contains - observation, discount, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb138b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ems, num_items\n",
    "# output: (DeviceArray(40, dtype=int32), DeviceArray(20, dtype=int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593903c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ems * num_items\n",
    "# DeviceArray(800, dtype=int32)  picking an id from this can be done using NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92c464",
   "metadata": {},
   "source": [
    "## PPO\n",
    "### Reference of implementation: \n",
    "https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8  ( follow first 3 parts to understand the code) \n",
    "https://github.com/ericyangyu/PPO-for-Beginners/blob/9abd435771aa84764d8d0d1f737fa39118b74019/ppo.py#L260 github repo\n",
    "\n",
    "### Clean RL videos to understand inside, but it is using gym wrappers alot, i didn't see that in jumanji. So, i decided to go with above reference.\n",
    "https://docs.cleanrl.dev/rl-algorithms/ppo/\n",
    "\n",
    "### Reference to understand RL concepts: \n",
    "https://spinningup.openai.com/en/latest/spinningup/rl_intro.html (basic concepts of RL important, will help us on how to change inside ppo to adjust 3D bins)\n",
    "\n",
    "\n",
    "### functions of jumanji that can be used : \n",
    "https://github.com/instadeepai/jumanji/blob/main/jumanji/environments/combinatorial/binpack/env.py\n",
    "reference to snake game : https://colab.research.google.com/github/instadeepai/jumanji/blob/main/examples/anakin_snake.ipynb#scrollTo=mNd-1Zgp5MGZ\n",
    "but this snake game is 2D, we are dealing with 3D\n",
    "\n",
    "### gym wrapper concept \n",
    "https://www.gymlibrary.dev/api/wrappers/\n",
    "\n",
    "### DQN \n",
    "https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class FeedForwardNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(FeedForwardNN, self).__init__()\n",
    "  def __init__(self, in_dim, out_dim):\n",
    "    super(FeedForwardNN, self).__init__()\n",
    "    self.layer1 = nn.Linear(in_dim, 64)\n",
    "    self.layer2 = nn.Linear(64, 64)\n",
    "    self.layer3 = nn.Linear(64, out_dim)\n",
    "  def forward(self, obs):\n",
    "  # Convert observation to tensor if it's a numpy array\n",
    "    if isinstance(obs, np.ndarray):\n",
    "      obs = torch.tensor(obs, dtype=torch.float)\n",
    "      print(\"inside tensor\")\n",
    "  \n",
    "    activation1 = F.relu(self.layer1(obs))\n",
    "    activation2 = F.relu(self.layer2(activation1))\n",
    "    output = self.layer3(activation2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b405f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### observation instead of state \n",
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "class PPO:\n",
    "  def __init__(self,env):\n",
    "    self._init_hyperparameters()\n",
    "    self.env = env\n",
    "    #####################################\n",
    "    self.obs_dim = env.obs_num_ems\n",
    "    self.act_dim = env.action_spec().shape[0]\n",
    "    ######################################\n",
    "    \n",
    "\n",
    "    #initiate actor and critic\n",
    "    self.actor = FeedForwardNN(self.obs_dim,self.act_dim)\n",
    "    self.critic = FeedForwardNN(self.obs_dim,1)\n",
    "\n",
    "      # Create our variable for the matrix.\n",
    "    # Note that I chose 0.5 for stdev arbitrarily.\n",
    "    self.cov_var = torch.full(size=(self.act_dim,), fill_value=0.5)\n",
    "    \n",
    "    # Create the covariance matrix\n",
    "    self.cov_mat = torch.diag(self.cov_var)\n",
    "    self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "    self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "    \n",
    "    \n",
    "  def _init_hyperparameters(self):\n",
    "    # Default values for hyperparameters, will need to change later.\n",
    "    self.timesteps_per_batch = 4800            # timesteps per batch\n",
    "    self.max_timesteps_per_episode = 1600      # timesteps per episode\n",
    "    self.gamma = 0.95\n",
    "    self.n_updates_per_iteration = 5\n",
    "    self.clip = 0.2 # As recommended by the paper\n",
    "    self.lr = 0.005\n",
    "\n",
    "  def compute_rtgs(self, batch_rews): \n",
    "    # The rewards-to-go (rtg) per episode per batch to return.\n",
    "    # The shape will be (num timesteps per episode)\n",
    "    batch_rtgs = []\n",
    "    # Iterate through each episode backwards to maintain same order\n",
    "    # in batch_rtgs\n",
    "    for ep_rews in reversed(batch_rews):\n",
    "      discounted_reward = 0 # The discounted reward so far\n",
    "      for rew in reversed(ep_rews):\n",
    "        discounted_reward = rew + discounted_reward * self.gamma\n",
    "        batch_rtgs.insert(0, discounted_reward)\n",
    "    # Convert the rewards-to-go into a tensor\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "    return batch_rtgs\n",
    "\n",
    "\n",
    "  def rollout(self):\n",
    "    # Batch data\n",
    "    batch_obs = []             # batch observations\n",
    "    batch_acts = []            # batch actions\n",
    "    batch_log_probs = []       # log probs of each action\n",
    "    batch_rews = []            # batch rewards\n",
    "    batch_rtgs = []            # batch rewards-to-go\n",
    "    batch_lens = []            # episodic lengths in batch\n",
    "    # Number of timesteps run so far this batch\n",
    "    t = 0 \n",
    "    while t < self.timesteps_per_batch:\n",
    "      # Rewards this episode\n",
    "      ep_rews = []\n",
    "      key = jax.random.PRNGKey(0)\n",
    "      ###############################\n",
    "      #jax.jit(env.reset)(key)\n",
    "      state, timestep = self.env.reset(key)\n",
    "      ###############################\n",
    "    \n",
    "      for ep_t in range(self.max_timesteps_per_episode):\n",
    "\n",
    "        # Increment timesteps ran this batch so far\n",
    "        t += 1\n",
    "        # Collect observation\n",
    "        ################################################\n",
    "        batch_obs.append(state)\n",
    "        num_ems, num_items = env.action_spec().num_values\n",
    "        ems_item_id = jax.random.choice(\n",
    "            key=key,\n",
    "            a=num_ems * num_items,\n",
    "            p=timestep.observation.action_mask.flatten(),\n",
    "          )\n",
    "        ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
    "\n",
    "        # Wrap the action as a jax array of shape (2,)\n",
    "        action = jnp.array([ems_id, item_id])\n",
    "        mean = self.actor(state)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        state,timestep = self.env.step(action)\n",
    "        rew = timestep.reward-1 \n",
    "        ##################################################\n",
    "        # Collect reward, action, and log prob\n",
    "        ep_rews.append(rew)\n",
    "        batch_acts.append(action)\n",
    "        batch_log_probs(log_prob.detach())\n",
    "      # Collect episodic length and rewards\n",
    "      batch_lens.append(ep_t + 1) # plus 1 because timestep starts at 0\n",
    "      batch_rews.append(ep_rews) \n",
    "      # Reshape data as tensors in the shape specified before returning\n",
    "    batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "    batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
    "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
    "    # ALG STEP #4\n",
    "    batch_rtgs = self.compute_rtgs(batch_rews)\n",
    "    # Return the batch data\n",
    "    return batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens\n",
    "\n",
    "  def learn(self, total_timesteps):\n",
    "    t_so_far = 0 # Timesteps simulated so far\n",
    "    while t_so_far < total_timesteps:              # ALG STEP 2\n",
    "      # Increment t_so_far somewhere below\n",
    "      # ALG STEP 3\n",
    "      batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
    "      # Calculate how many timesteps we collected this batch   \n",
    "      t_so_far += np.sum(batch_lens)\n",
    "      # Calculate V_{phi, k}\n",
    "      V, _ = self.evaluate(batch_obs, batch_acts)\n",
    "      # ALG STEP 5\n",
    "      # Calculate advantage\n",
    "      A_k = batch_rtgs - V.detach()\n",
    "      # Normalize advantages\n",
    "      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "      for _ in range(self.n_updates_per_iteration):\n",
    "        # Calculate V_phi and pi_theta(a_t | s_t)    \n",
    "        V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "        # Calculate ratios\n",
    "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "        # Calculate surrogate losses\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
    "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "        # Calculate gradients and perform backward propagation for actor \n",
    "        # network\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "        # Calculate gradients and perform backward propagation for critic network    \n",
    "        self.critic_optim.zero_grad()    \n",
    "        critic_loss.backward()    \n",
    "        self.critic_optim.step()\n",
    "    \n",
    "  def evaluate(self, batch_obs,batch_acts):\n",
    "    # Query critic network for a value V for each obs in batch_obs.\n",
    "    V = self.critic(batch_obs).squeeze()\n",
    "    # Calculate the log probabilities of batch actions using most \n",
    "    # recent actor network.\n",
    "    # This segment of code is similar to that in get_action()\n",
    "    mean = self.actor(batch_obs)\n",
    "    dist = MultivariateNormal(mean, self.cov_mat)\n",
    "    log_probs = dist.log_prob(batch_acts)\n",
    "    # Return predicted values V and log probs log_probs\n",
    "    return V, log_probs\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb90fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(env)\n",
    "model.learn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67db49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
