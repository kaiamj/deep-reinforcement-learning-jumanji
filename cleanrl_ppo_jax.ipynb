{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpgzkfF2dCgYYfZwPJD45U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiamj/bdrp/blob/main/cleanrl_ppo_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.  code source : https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari_envpool_xla_jax.py \n",
        "2.  dependencies : https://docs.cleanrl.dev/get-started/installation/#optional-dependencies \n",
        "3. documentation : https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_atari_envpool_xla_jaxpy  \n",
        "4. to make parser work in jupyter notebook : parser.add_argument('-f')  \n",
        "\n"
      ],
      "metadata": {
        "id": "hQhCyWNBmp2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flax\n",
        "!pip install envpool\n",
        "!pip install atari\n",
        "!pip install pybullet\n",
        "!pip install mujoco_py\n",
        "!pip install procgen\n",
        "!pip install envpool\n",
        "!pip install pettingzoo\n",
        "!pip install jax\n",
        "!pip install optuna\n",
        "!pip install docs\n",
        "#!pip install cloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3i7CVLjEisf3",
        "outputId": "af67b002-7caf-40a5-8ac4-1b5e19ef36ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: envpool in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: dm-env>=1.4 in /usr/local/lib/python3.8/dist-packages (from envpool) (1.6)\n",
            "Requirement already satisfied: gymnasium>=0.26 in /usr/local/lib/python3.8/dist-packages (from envpool) (0.27.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from envpool) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from envpool) (1.21.6)\n",
            "Requirement already satisfied: treevalue>=1.4 in /usr/local/lib/python3.8/dist-packages (from envpool) (1.4.3)\n",
            "Requirement already satisfied: types-protobuf>=3.17.3 in /usr/local/lib/python3.8/dist-packages (from envpool) (4.21.0.2)\n",
            "Requirement already satisfied: gym>=0.18 in /usr/local/lib/python3.8/dist-packages (from envpool) (0.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from envpool) (21.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from dm-env>=1.4->envpool) (0.1.8)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from dm-env>=1.4->envpool) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.18->envpool) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.18->envpool) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.18->envpool) (2.2.0)\n",
            "Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26->envpool) (0.2.0)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26->envpool) (0.0.1)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26->envpool) (0.2.0)\n",
            "Requirement already satisfied: hbutils>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (0.8.0)\n",
            "Requirement already satisfied: enum-tools in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (0.9.0.post1)\n",
            "Requirement already satisfied: click>=7.1.0 in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (7.1.2)\n",
            "Requirement already satisfied: dill~=0.3.4 in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (0.3.6)\n",
            "Requirement already satisfied: graphviz~=0.17 in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (0.20.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->envpool) (3.0.9)\n",
            "Requirement already satisfied: setuptools>=50.0 in /usr/local/lib/python3.8/dist-packages (from hbutils>=0.0.1->treevalue>=1.4->envpool) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.18->envpool) (3.11.0)\n",
            "Requirement already satisfied: pygments>=2.6.1 in /usr/local/lib/python3.8/dist-packages (from enum-tools->treevalue>=1.4->envpool) (2.6.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement atari (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for atari\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mujoco_py\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.8/dist-packages (from mujoco_py) (2.9.0)\n",
            "Collecting fasteners~=0.15\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.8/dist-packages (from mujoco_py) (1.15.1)\n",
            "Collecting glfw>=1.4.0\n",
            "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.8/dist-packages (from mujoco_py) (0.29.32)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from mujoco_py) (1.21.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.10->mujoco_py) (2.21)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio>=2.1.2->mujoco_py) (7.1.2)\n",
            "Installing collected packages: glfw, fasteners, mujoco_py\n",
            "Successfully installed fasteners-0.18 glfw-2.5.5 mujoco_py-2.1.2.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting procgen\n",
            "  Downloading procgen-0.10.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from procgen) (3.9.0)\n",
            "Collecting gym3<1.0.0,>=0.3.3\n",
            "  Downloading gym3-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from procgen) (1.21.6)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from procgen) (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (2.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (6.0.0)\n",
            "Collecting moderngl<6.0.0,>=5.5.4\n",
            "  Downloading moderngl-5.7.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (710 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m710.5/710.5 KB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.9.0)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.15.1)\n",
            "Collecting glfw<2.0.0,>=1.8.6\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio-ffmpeg<0.4.0,>=0.3.0\n",
            "  Downloading imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.21)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym<1.0.0,>=0.15.0->procgen) (3.11.0)\n",
            "Collecting glcontext<3,>=2.3.6\n",
            "  Downloading glcontext-2.3.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg, glfw, glcontext, moderngl, gym3, procgen\n",
            "  Attempting uninstall: glfw\n",
            "    Found existing installation: glfw 2.5.5\n",
            "    Uninstalling glfw-2.5.5:\n",
            "      Successfully uninstalled glfw-2.5.5\n",
            "Successfully installed glcontext-2.3.7 glfw-1.12.0 gym3-0.3.3 imageio-ffmpeg-0.3.0 moderngl-5.7.4 procgen-0.10.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: envpool in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: gym>=0.18 in /usr/local/lib/python3.8/dist-packages (from envpool) (0.25.2)\n",
            "Requirement already satisfied: dm-env>=1.4 in /usr/local/lib/python3.8/dist-packages (from envpool) (1.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from envpool) (21.3)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from envpool) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from envpool) (4.4.0)\n",
            "Requirement already satisfied: gymnasium>=0.26 in /usr/local/lib/python3.8/dist-packages (from envpool) (0.27.0)\n",
            "Requirement already satisfied: types-protobuf>=3.17.3 in /usr/local/lib/python3.8/dist-packages (from envpool) (4.21.0.2)\n",
            "Requirement already satisfied: treevalue>=1.4 in /usr/local/lib/python3.8/dist-packages (from envpool) (1.4.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from dm-env>=1.4->envpool) (0.1.8)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from dm-env>=1.4->envpool) (1.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.18->envpool) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.18->envpool) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.18->envpool) (2.2.0)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26->envpool) (0.0.1)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26->envpool) (0.2.0)\n",
            "Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26->envpool) (0.2.0)\n",
            "Requirement already satisfied: hbutils>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (0.8.0)\n",
            "Requirement already satisfied: enum-tools in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (0.9.0.post1)\n",
            "Requirement already satisfied: dill~=0.3.4 in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (0.3.6)\n",
            "Requirement already satisfied: click>=7.1.0 in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (7.1.2)\n",
            "Requirement already satisfied: graphviz~=0.17 in /usr/local/lib/python3.8/dist-packages (from treevalue>=1.4->envpool) (0.20.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->envpool) (3.0.9)\n",
            "Requirement already satisfied: setuptools>=50.0 in /usr/local/lib/python3.8/dist-packages (from hbutils>=0.0.1->treevalue>=1.4->envpool) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.18->envpool) (3.11.0)\n",
            "Requirement already satisfied: pygments>=2.6.1 in /usr/local/lib/python3.8/dist-packages (from enum-tools->treevalue>=1.4->envpool) (2.6.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pettingzoo\n",
            "  Downloading PettingZoo-1.22.3-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from pettingzoo) (1.21.6)\n",
            "Requirement already satisfied: gymnasium>=0.26.0 in /usr/local/lib/python3.8/dist-packages (from pettingzoo) (0.27.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (2.2.0)\n",
            "Requirement already satisfied: jax-jumpy>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (0.2.0)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (4.4.0)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from gymnasium>=0.26.0->pettingzoo) (0.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gymnasium>=0.26.0->pettingzoo) (3.11.0)\n",
            "Installing collected packages: pettingzoo\n",
            "Successfully installed pettingzoo-1.22.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.8/dist-packages (0.3.25)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from jax) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.5-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (21.3)\n",
            "Collecting importlib-metadata<5.0.0\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.46)\n",
            "Collecting cliff\n",
            "  Downloading cliff-4.1.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.9.1-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.4/210.4 KB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.10.2)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<5.0.0->optuna) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.1/147.1 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-4.1.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.2.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=83b5c22c05d3f146b344d42e868fa966564238be8ffee1daa29c36ac650c416a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, Mako, importlib-metadata, colorlog, cmd2, cmaes, autopage, stevedore, alembic, cliff, optuna\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "Successfully installed Mako-1.2.4 alembic-1.9.1 autopage-0.5.1 cliff-4.1.0 cmaes-0.9.1 cmd2-2.4.2 colorlog-6.7.0 importlib-metadata-4.13.0 optuna-3.0.5 pbr-5.11.1 pyperclip-1.8.2 stevedore-4.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting docs\n",
            "  Downloading docs-0.1.0.tar.gz (717 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docs\n",
            "  Building wheel for docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docs: filename=docs-0.1.0-py3-none-any.whl size=1040 sha256=8820cf7f47116dde176ac4841239da95f3aec31115811347e852762eb8a46bf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/cf/0f/ce1917865478c8d3429d37bd0a7ef86d5e67cedc92becb29f4\n",
            "Successfully built docs\n",
            "Installing collected packages: docs\n",
            "Successfully installed docs-0.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cloud\n",
            "  Downloading cloud-2.8.5.tar.gz (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9bvm0fIiQo4",
        "outputId": "12a97327-7c7e-4d42-d42b-2ad604ecd06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=1024, avg_episodic_return=0.0\n",
            "SPS: 8\n",
            "global_step=2048, avg_episodic_return=0.0\n",
            "SPS: 11\n",
            "global_step=3072, avg_episodic_return=0.0\n",
            "SPS: 16\n",
            "global_step=4096, avg_episodic_return=0.0\n",
            "SPS: 21\n",
            "global_step=5120, avg_episodic_return=0.0\n",
            "SPS: 27\n",
            "global_step=6144, avg_episodic_return=0.0\n",
            "SPS: 32\n",
            "global_step=7168, avg_episodic_return=-15.625\n",
            "SPS: 37\n",
            "global_step=8192, avg_episodic_return=-20.75\n",
            "SPS: 43\n",
            "global_step=9216, avg_episodic_return=-20.75\n",
            "SPS: 48\n",
            "global_step=10240, avg_episodic_return=-20.75\n",
            "SPS: 53\n",
            "global_step=11264, avg_episodic_return=-20.75\n",
            "SPS: 58\n",
            "global_step=12288, avg_episodic_return=-20.75\n",
            "SPS: 63\n",
            "global_step=13312, avg_episodic_return=-20.75\n",
            "SPS: 68\n",
            "global_step=14336, avg_episodic_return=-20.75\n",
            "SPS: 73\n",
            "global_step=15360, avg_episodic_return=-20.625\n",
            "SPS: 78\n",
            "global_step=16384, avg_episodic_return=-20.625\n",
            "SPS: 83\n",
            "global_step=17408, avg_episodic_return=-20.625\n",
            "SPS: 88\n",
            "global_step=18432, avg_episodic_return=-20.625\n",
            "SPS: 93\n",
            "global_step=19456, avg_episodic_return=-20.625\n",
            "SPS: 98\n",
            "global_step=20480, avg_episodic_return=-20.625\n",
            "SPS: 103\n",
            "global_step=21504, avg_episodic_return=-20.375\n",
            "SPS: 107\n",
            "global_step=22528, avg_episodic_return=-20.375\n",
            "SPS: 111\n",
            "global_step=23552, avg_episodic_return=-20.375\n",
            "SPS: 116\n",
            "global_step=24576, avg_episodic_return=-20.375\n",
            "SPS: 120\n",
            "global_step=25600, avg_episodic_return=-20.375\n",
            "SPS: 125\n",
            "global_step=26624, avg_episodic_return=-20.375\n",
            "SPS: 129\n",
            "global_step=27648, avg_episodic_return=-20.375\n",
            "SPS: 133\n",
            "global_step=28672, avg_episodic_return=-20.375\n",
            "SPS: 138\n",
            "global_step=29696, avg_episodic_return=-20.25\n",
            "SPS: 142\n",
            "global_step=30720, avg_episodic_return=-20.25\n",
            "SPS: 146\n",
            "global_step=31744, avg_episodic_return=-20.25\n",
            "SPS: 151\n",
            "global_step=32768, avg_episodic_return=-20.25\n",
            "SPS: 155\n",
            "global_step=33792, avg_episodic_return=-20.25\n",
            "SPS: 159\n",
            "global_step=34816, avg_episodic_return=-20.25\n",
            "SPS: 163\n",
            "global_step=35840, avg_episodic_return=-20.25\n",
            "SPS: 167\n",
            "global_step=36864, avg_episodic_return=-20.25\n",
            "SPS: 172\n",
            "global_step=37888, avg_episodic_return=-20.625\n",
            "SPS: 176\n",
            "global_step=38912, avg_episodic_return=-20.75\n",
            "SPS: 180\n",
            "global_step=39936, avg_episodic_return=-20.75\n",
            "SPS: 184\n",
            "global_step=40960, avg_episodic_return=-20.75\n",
            "SPS: 188\n",
            "global_step=41984, avg_episodic_return=-20.625\n",
            "SPS: 192\n",
            "global_step=43008, avg_episodic_return=-20.625\n",
            "SPS: 196\n",
            "global_step=44032, avg_episodic_return=-20.625\n",
            "SPS: 200\n",
            "global_step=45056, avg_episodic_return=-20.625\n",
            "SPS: 204\n",
            "global_step=46080, avg_episodic_return=-20.375\n",
            "SPS: 207\n",
            "global_step=47104, avg_episodic_return=-20.25\n",
            "SPS: 211\n",
            "global_step=48128, avg_episodic_return=-20.25\n",
            "SPS: 215\n",
            "global_step=49152, avg_episodic_return=-20.25\n",
            "SPS: 219\n",
            "global_step=50176, avg_episodic_return=-20.25\n",
            "SPS: 223\n",
            "global_step=51200, avg_episodic_return=-20.25\n",
            "SPS: 226\n",
            "global_step=52224, avg_episodic_return=-20.5\n",
            "SPS: 230\n",
            "global_step=53248, avg_episodic_return=-20.625\n",
            "SPS: 234\n",
            "global_step=54272, avg_episodic_return=-20.625\n",
            "SPS: 238\n",
            "global_step=55296, avg_episodic_return=-20.625\n",
            "SPS: 242\n",
            "global_step=56320, avg_episodic_return=-20.625\n",
            "SPS: 245\n",
            "global_step=57344, avg_episodic_return=-20.75\n",
            "SPS: 249\n",
            "global_step=58368, avg_episodic_return=-20.625\n",
            "SPS: 252\n",
            "global_step=59392, avg_episodic_return=-20.625\n",
            "SPS: 256\n",
            "global_step=60416, avg_episodic_return=-20.5\n",
            "SPS: 259\n",
            "global_step=61440, avg_episodic_return=-20.5\n",
            "SPS: 263\n",
            "global_step=62464, avg_episodic_return=-20.5\n",
            "SPS: 267\n",
            "global_step=63488, avg_episodic_return=-20.5\n",
            "SPS: 270\n",
            "global_step=64512, avg_episodic_return=-20.375\n",
            "SPS: 273\n",
            "global_step=65536, avg_episodic_return=-20.375\n",
            "SPS: 277\n",
            "global_step=66560, avg_episodic_return=-20.5\n",
            "SPS: 280\n",
            "global_step=67584, avg_episodic_return=-20.375\n",
            "SPS: 284\n",
            "global_step=68608, avg_episodic_return=-20.375\n",
            "SPS: 287\n",
            "global_step=69632, avg_episodic_return=-20.5\n",
            "SPS: 290\n",
            "global_step=70656, avg_episodic_return=-20.625\n",
            "SPS: 294\n",
            "global_step=71680, avg_episodic_return=-20.625\n",
            "SPS: 297\n",
            "global_step=72704, avg_episodic_return=-20.625\n",
            "SPS: 300\n",
            "global_step=73728, avg_episodic_return=-20.625\n",
            "SPS: 304\n",
            "global_step=74752, avg_episodic_return=-20.5\n",
            "SPS: 307\n",
            "global_step=75776, avg_episodic_return=-20.5\n",
            "SPS: 310\n",
            "global_step=76800, avg_episodic_return=-20.5\n",
            "SPS: 313\n",
            "global_step=77824, avg_episodic_return=-20.25\n",
            "SPS: 316\n",
            "global_step=78848, avg_episodic_return=-20.25\n",
            "SPS: 320\n",
            "global_step=79872, avg_episodic_return=-20.25\n",
            "SPS: 323\n",
            "global_step=80896, avg_episodic_return=-20.375\n",
            "SPS: 326\n",
            "global_step=81920, avg_episodic_return=-20.5\n",
            "SPS: 329\n",
            "global_step=82944, avg_episodic_return=-20.5\n",
            "SPS: 332\n",
            "global_step=83968, avg_episodic_return=-20.625\n",
            "SPS: 335\n",
            "global_step=84992, avg_episodic_return=-20.5\n",
            "SPS: 338\n",
            "global_step=86016, avg_episodic_return=-20.5\n",
            "SPS: 341\n",
            "global_step=87040, avg_episodic_return=-20.625\n",
            "SPS: 344\n",
            "global_step=88064, avg_episodic_return=-20.5\n",
            "SPS: 347\n",
            "global_step=89088, avg_episodic_return=-20.5\n",
            "SPS: 350\n",
            "global_step=90112, avg_episodic_return=-20.5\n",
            "SPS: 353\n",
            "global_step=91136, avg_episodic_return=-20.375\n",
            "SPS: 356\n",
            "global_step=92160, avg_episodic_return=-20.625\n",
            "SPS: 359\n",
            "global_step=93184, avg_episodic_return=-20.625\n",
            "SPS: 362\n",
            "global_step=94208, avg_episodic_return=-20.625\n",
            "SPS: 365\n",
            "global_step=95232, avg_episodic_return=-20.75\n",
            "SPS: 368\n",
            "global_step=96256, avg_episodic_return=-20.625\n",
            "SPS: 371\n",
            "global_step=97280, avg_episodic_return=-20.625\n",
            "SPS: 374\n",
            "global_step=98304, avg_episodic_return=-20.75\n",
            "SPS: 376\n",
            "global_step=99328, avg_episodic_return=-20.625\n",
            "SPS: 379\n"
          ]
        }
      ],
      "source": [
        "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_atari_envpool_xla_jaxpy\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "from typing import Sequence\n",
        "\n",
        "os.environ[\n",
        "    \"XLA_PYTHON_CLIENT_MEM_FRACTION\"\n",
        "] = \"0.7\"  # see https://github.com/google/jax/discussions/6332#discussioncomment-1279991\n",
        "\n",
        "import envpool\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import gym\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from flax.training.train_state import TrainState\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # fmt: off\n",
        "    parser = argparse.ArgumentParser()\n",
        "    #########################################\n",
        "    parser.add_argument('-f')  # to make parser work in jupyter notebook\n",
        "    #########################################\n",
        "    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(\"__file__\"),\n",
        "        help=\"the name of this experiment\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=1,\n",
        "        help=\"seed of the experiment\")\n",
        "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
        "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, cuda will be enabled by default\")\n",
        "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
        "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
        "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n",
        "        help=\"the wandb's project name\")\n",
        "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
        "        help=\"the entity (team) of wandb's project\")\n",
        "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
        "        help=\"whether to capture videos of the agent performances (check out `videos` folder)\")\n",
        "\n",
        "    # Algorithm specific arguments\n",
        "    parser.add_argument(\"--env-id\", type=str, default=\"Pong-v5\",\n",
        "        help=\"the id of the environment\")\n",
        "    parser.add_argument(\"--total-timesteps\", type=int, default=100000,\n",
        "        help=\"total timesteps of the experiments\")\n",
        "    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n",
        "        help=\"the learning rate of the optimizer\")\n",
        "    parser.add_argument(\"--num-envs\", type=int, default=8,\n",
        "        help=\"the number of parallel game environments\")\n",
        "    parser.add_argument(\"--num-steps\", type=int, default=128,\n",
        "        help=\"the number of steps to run in each environment per policy rollout\")\n",
        "    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Toggle learning rate annealing for policy and value networks\")\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
        "        help=\"the discount factor gamma\")\n",
        "    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n",
        "        help=\"the lambda for the general advantage estimation\")\n",
        "    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n",
        "        help=\"the number of mini-batches\")\n",
        "    parser.add_argument(\"--update-epochs\", type=int, default=4,\n",
        "        help=\"the K epochs to update the policy\")\n",
        "    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
        "        help=\"Toggles advantages normalization\")\n",
        "    parser.add_argument(\"--clip-coef\", type=float, default=0.1,\n",
        "        help=\"the surrogate clipping coefficient\")\n",
        "    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n",
        "        help=\"coefficient of the entropy\")\n",
        "    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n",
        "        help=\"coefficient of the value function\")\n",
        "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
        "        help=\"the maximum norm for the gradient clipping\")\n",
        "    parser.add_argument(\"--target-kl\", type=float, default=None,\n",
        "        help=\"the target KL divergence threshold\")\n",
        "    args = parser.parse_args()\n",
        "    args.batch_size = int(args.num_envs * args.num_steps)\n",
        "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
        "    args.num_updates = args.total_timesteps // args.batch_size\n",
        "    # fmt: on\n",
        "    return args\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = jnp.transpose(x, (0, 2, 3, 1))\n",
        "        x = x / (255.0)\n",
        "        x = nn.Conv(\n",
        "            32,\n",
        "            kernel_size=(8, 8),\n",
        "            strides=(4, 4),\n",
        "            padding=\"VALID\",\n",
        "            kernel_init=orthogonal(np.sqrt(2)),\n",
        "            bias_init=constant(0.0),\n",
        "        )(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Conv(\n",
        "            64,\n",
        "            kernel_size=(4, 4),\n",
        "            strides=(2, 2),\n",
        "            padding=\"VALID\",\n",
        "            kernel_init=orthogonal(np.sqrt(2)),\n",
        "            bias_init=constant(0.0),\n",
        "        )(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Conv(\n",
        "            64,\n",
        "            kernel_size=(3, 3),\n",
        "            strides=(1, 1),\n",
        "            padding=\"VALID\",\n",
        "            kernel_init=orthogonal(np.sqrt(2)),\n",
        "            bias_init=constant(0.0),\n",
        "        )(x)\n",
        "        x = nn.relu(x)\n",
        "        x = x.reshape((x.shape[0], -1))\n",
        "        x = nn.Dense(512, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
        "        x = nn.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        return nn.Dense(1, kernel_init=orthogonal(1), bias_init=constant(0.0))(x)\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    action_dim: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        return nn.Dense(self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0))(x)\n",
        "\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class AgentParams:\n",
        "    network_params: flax.core.FrozenDict\n",
        "    actor_params: flax.core.FrozenDict\n",
        "    critic_params: flax.core.FrozenDict\n",
        "\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class Storage:\n",
        "    obs: jnp.array\n",
        "    actions: jnp.array\n",
        "    logprobs: jnp.array\n",
        "    dones: jnp.array\n",
        "    values: jnp.array\n",
        "    advantages: jnp.array\n",
        "    returns: jnp.array\n",
        "    rewards: jnp.array\n",
        "\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class EpisodeStatistics:\n",
        "    episode_returns: jnp.array\n",
        "    episode_lengths: jnp.array\n",
        "    returned_episode_returns: jnp.array\n",
        "    returned_episode_lengths: jnp.array\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "    if args.track:\n",
        "        import wandb\n",
        "\n",
        "        wandb.init(\n",
        "            project=args.wandb_project_name,\n",
        "            entity=args.wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config=vars(args),\n",
        "            name=run_name,\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    key = jax.random.PRNGKey(args.seed)\n",
        "    key, network_key, actor_key, critic_key = jax.random.split(key, 4)\n",
        "\n",
        "    # env setup\n",
        "    envs = envpool.make(\n",
        "        args.env_id,\n",
        "        env_type=\"gym\",\n",
        "        num_envs=args.num_envs,\n",
        "        episodic_life=True,\n",
        "        reward_clip=True,\n",
        "        seed=args.seed,\n",
        "    )\n",
        "    envs.num_envs = args.num_envs\n",
        "    envs.single_action_space = envs.action_space\n",
        "    envs.single_observation_space = envs.observation_space\n",
        "    envs.is_vector_env = True\n",
        "    episode_stats = EpisodeStatistics(\n",
        "        episode_returns=jnp.zeros(args.num_envs, dtype=jnp.float32),\n",
        "        episode_lengths=jnp.zeros(args.num_envs, dtype=jnp.int32),\n",
        "        returned_episode_returns=jnp.zeros(args.num_envs, dtype=jnp.float32),\n",
        "        returned_episode_lengths=jnp.zeros(args.num_envs, dtype=jnp.int32),\n",
        "    )\n",
        "    handle, recv, send, step_env = envs.xla()\n",
        "\n",
        "    def step_env_wrappeed(episode_stats, handle, action):\n",
        "        handle, (next_obs, reward, next_done, info) = step_env(handle, action)\n",
        "        new_episode_return = episode_stats.episode_returns + info[\"reward\"]\n",
        "        new_episode_length = episode_stats.episode_lengths + 1\n",
        "        episode_stats = episode_stats.replace(\n",
        "            episode_returns=(new_episode_return) * (1 - info[\"terminated\"]) * (1 - info[\"TimeLimit.truncated\"]),\n",
        "            episode_lengths=(new_episode_length) * (1 - info[\"terminated\"]) * (1 - info[\"TimeLimit.truncated\"]),\n",
        "            # only update the `returned_episode_returns` if the episode is done\n",
        "            returned_episode_returns=jnp.where(\n",
        "                info[\"terminated\"] + info[\"TimeLimit.truncated\"], new_episode_return, episode_stats.returned_episode_returns\n",
        "            ),\n",
        "            returned_episode_lengths=jnp.where(\n",
        "                info[\"terminated\"] + info[\"TimeLimit.truncated\"], new_episode_length, episode_stats.returned_episode_lengths\n",
        "            ),\n",
        "        )\n",
        "        return episode_stats, handle, (next_obs, reward, next_done, info)\n",
        "\n",
        "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "    def linear_schedule(count):\n",
        "        # anneal learning rate linearly after one training iteration which contains\n",
        "        # (args.num_minibatches * args.update_epochs) gradient updates\n",
        "        frac = 1.0 - (count // (args.num_minibatches * args.update_epochs)) / args.num_updates\n",
        "        return args.learning_rate * frac\n",
        "\n",
        "    network = Network()\n",
        "    actor = Actor(action_dim=envs.single_action_space.n)\n",
        "    critic = Critic()\n",
        "    network_params = network.init(network_key, np.array([envs.single_observation_space.sample()]))\n",
        "    agent_state = TrainState.create(\n",
        "        apply_fn=None,\n",
        "        params=AgentParams(\n",
        "            network_params,\n",
        "            actor.init(actor_key, network.apply(network_params, np.array([envs.single_observation_space.sample()]))),\n",
        "            critic.init(critic_key, network.apply(network_params, np.array([envs.single_observation_space.sample()]))),\n",
        "        ),\n",
        "        tx=optax.chain(\n",
        "            optax.clip_by_global_norm(args.max_grad_norm),\n",
        "            optax.inject_hyperparams(optax.adam)(\n",
        "                learning_rate=linear_schedule if args.anneal_lr else args.learning_rate, eps=1e-5\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "    network.apply = jax.jit(network.apply)\n",
        "    actor.apply = jax.jit(actor.apply)\n",
        "    critic.apply = jax.jit(critic.apply)\n",
        "\n",
        "    # ALGO Logic: Storage setup\n",
        "    storage = Storage(\n",
        "        obs=jnp.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape),\n",
        "        actions=jnp.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape, dtype=jnp.int32),\n",
        "        logprobs=jnp.zeros((args.num_steps, args.num_envs)),\n",
        "        dones=jnp.zeros((args.num_steps, args.num_envs)),\n",
        "        values=jnp.zeros((args.num_steps, args.num_envs)),\n",
        "        advantages=jnp.zeros((args.num_steps, args.num_envs)),\n",
        "        returns=jnp.zeros((args.num_steps, args.num_envs)),\n",
        "        rewards=jnp.zeros((args.num_steps, args.num_envs)),\n",
        "    )\n",
        "\n",
        "    @jax.jit\n",
        "    def get_action_and_value(\n",
        "        agent_state: TrainState,\n",
        "        next_obs: np.ndarray,\n",
        "        next_done: np.ndarray,\n",
        "        storage: Storage,\n",
        "        step: int,\n",
        "        key: jax.random.PRNGKey,\n",
        "    ):\n",
        "        \"\"\"sample action, calculate value, logprob, entropy, and update storage\"\"\"\n",
        "        hidden = network.apply(agent_state.params.network_params, next_obs)\n",
        "        logits = actor.apply(agent_state.params.actor_params, hidden)\n",
        "        # sample action: Gumbel-softmax trick\n",
        "        # see https://stats.stackexchange.com/questions/359442/sampling-from-a-categorical-distribution\n",
        "        key, subkey = jax.random.split(key)\n",
        "        u = jax.random.uniform(subkey, shape=logits.shape)\n",
        "        action = jnp.argmax(logits - jnp.log(-jnp.log(u)), axis=1)\n",
        "        logprob = jax.nn.log_softmax(logits)[jnp.arange(action.shape[0]), action]\n",
        "        value = critic.apply(agent_state.params.critic_params, hidden)\n",
        "        storage = storage.replace(\n",
        "            obs=storage.obs.at[step].set(next_obs),\n",
        "            dones=storage.dones.at[step].set(next_done),\n",
        "            actions=storage.actions.at[step].set(action),\n",
        "            logprobs=storage.logprobs.at[step].set(logprob),\n",
        "            values=storage.values.at[step].set(value.squeeze()),\n",
        "        )\n",
        "        return storage, action, key\n",
        "\n",
        "    @jax.jit\n",
        "    def get_action_and_value2(\n",
        "        params: flax.core.FrozenDict,\n",
        "        x: np.ndarray,\n",
        "        action: np.ndarray,\n",
        "    ):\n",
        "        \"\"\"calculate value, logprob of supplied `action`, and entropy\"\"\"\n",
        "        hidden = network.apply(params.network_params, x)\n",
        "        logits = actor.apply(params.actor_params, hidden)\n",
        "        logprob = jax.nn.log_softmax(logits)[jnp.arange(action.shape[0]), action]\n",
        "        # normalize the logits https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\n",
        "        logits = logits - jax.scipy.special.logsumexp(logits, axis=-1, keepdims=True)\n",
        "        logits = logits.clip(min=jnp.finfo(logits.dtype).min)\n",
        "        p_log_p = logits * jax.nn.softmax(logits)\n",
        "        entropy = -p_log_p.sum(-1)\n",
        "        value = critic.apply(params.critic_params, hidden).squeeze()\n",
        "        return logprob, entropy, value\n",
        "\n",
        "    @jax.jit\n",
        "    def compute_gae(\n",
        "        agent_state: TrainState,\n",
        "        next_obs: np.ndarray,\n",
        "        next_done: np.ndarray,\n",
        "        storage: Storage,\n",
        "    ):\n",
        "        storage = storage.replace(advantages=storage.advantages.at[:].set(0.0))\n",
        "        next_value = critic.apply(\n",
        "            agent_state.params.critic_params, network.apply(agent_state.params.network_params, next_obs)\n",
        "        ).squeeze()\n",
        "        lastgaelam = 0\n",
        "        for t in reversed(range(args.num_steps)):\n",
        "            if t == args.num_steps - 1:\n",
        "                nextnonterminal = 1.0 - next_done\n",
        "                nextvalues = next_value\n",
        "            else:\n",
        "                nextnonterminal = 1.0 - storage.dones[t + 1]\n",
        "                nextvalues = storage.values[t + 1]\n",
        "            delta = storage.rewards[t] + args.gamma * nextvalues * nextnonterminal - storage.values[t]\n",
        "            lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
        "            storage = storage.replace(advantages=storage.advantages.at[t].set(lastgaelam))\n",
        "        storage = storage.replace(returns=storage.advantages + storage.values)\n",
        "        return storage\n",
        "\n",
        "    @jax.jit\n",
        "    def update_ppo(\n",
        "        agent_state: TrainState,\n",
        "        storage: Storage,\n",
        "        key: jax.random.PRNGKey,\n",
        "    ):\n",
        "        b_obs = storage.obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "        b_logprobs = storage.logprobs.reshape(-1)\n",
        "        b_actions = storage.actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "        b_advantages = storage.advantages.reshape(-1)\n",
        "        b_returns = storage.returns.reshape(-1)\n",
        "\n",
        "        def ppo_loss(params, x, a, logp, mb_advantages, mb_returns):\n",
        "            newlogprob, entropy, newvalue = get_action_and_value2(params, x, a)\n",
        "            logratio = newlogprob - logp\n",
        "            ratio = jnp.exp(logratio)\n",
        "            approx_kl = ((ratio - 1) - logratio).mean()\n",
        "\n",
        "            if args.norm_adv:\n",
        "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "            # Policy loss\n",
        "            pg_loss1 = -mb_advantages * ratio\n",
        "            pg_loss2 = -mb_advantages * jnp.clip(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
        "            pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "            # Value loss\n",
        "            v_loss = 0.5 * ((newvalue - mb_returns) ** 2).mean()\n",
        "\n",
        "            entropy_loss = entropy.mean()\n",
        "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
        "            return loss, (pg_loss, v_loss, entropy_loss, jax.lax.stop_gradient(approx_kl))\n",
        "\n",
        "        ppo_loss_grad_fn = jax.value_and_grad(ppo_loss, has_aux=True)\n",
        "        for _ in range(args.update_epochs):\n",
        "            key, subkey = jax.random.split(key)\n",
        "            b_inds = jax.random.permutation(subkey, args.batch_size, independent=True)\n",
        "            for start in range(0, args.batch_size, args.minibatch_size):\n",
        "                end = start + args.minibatch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "                (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(\n",
        "                    agent_state.params,\n",
        "                    b_obs[mb_inds],\n",
        "                    b_actions[mb_inds],\n",
        "                    b_logprobs[mb_inds],\n",
        "                    b_advantages[mb_inds],\n",
        "                    b_returns[mb_inds],\n",
        "                )\n",
        "                agent_state = agent_state.apply_gradients(grads=grads)\n",
        "        return agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, key\n",
        "\n",
        "    # TRY NOT TO MODIFY: start the game\n",
        "    global_step = 0\n",
        "    start_time = time.time()\n",
        "    next_obs = envs.reset()\n",
        "    next_done = np.zeros(args.num_envs)\n",
        "\n",
        "    @jax.jit\n",
        "    def rollout(agent_state, episode_stats, next_obs, next_done, storage, key, handle, global_step):\n",
        "        for step in range(0, args.num_steps):\n",
        "            global_step += 1 * args.num_envs\n",
        "            storage, action, key = get_action_and_value(agent_state, next_obs, next_done, storage, step, key)\n",
        "\n",
        "            # TRY NOT TO MODIFY: execute the game and log data.\n",
        "            episode_stats, handle, (next_obs, reward, next_done, _) = step_env_wrappeed(episode_stats, handle, action)\n",
        "            storage = storage.replace(rewards=storage.rewards.at[step].set(reward))\n",
        "        return agent_state, episode_stats, next_obs, next_done, storage, key, handle, global_step\n",
        "\n",
        "    for update in range(1, args.num_updates + 1):\n",
        "        update_time_start = time.time()\n",
        "        agent_state, episode_stats, next_obs, next_done, storage, key, handle, global_step = rollout(\n",
        "            agent_state, episode_stats, next_obs, next_done, storage, key, handle, global_step\n",
        "        )\n",
        "        storage = compute_gae(agent_state, next_obs, next_done, storage)\n",
        "        agent_state, loss, pg_loss, v_loss, entropy_loss, approx_kl, key = update_ppo(\n",
        "            agent_state,\n",
        "            storage,\n",
        "            key,\n",
        "        )\n",
        "        avg_episodic_return = np.mean(jax.device_get(episode_stats.returned_episode_returns))\n",
        "        print(f\"global_step={global_step}, avg_episodic_return={avg_episodic_return}\")\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        writer.add_scalar(\"charts/avg_episodic_return\", avg_episodic_return, global_step)\n",
        "        writer.add_scalar(\n",
        "            \"charts/avg_episodic_length\", np.mean(jax.device_get(episode_stats.returned_episode_lengths)), global_step\n",
        "        )\n",
        "        writer.add_scalar(\"charts/learning_rate\", agent_state.opt_state[1].hyperparams[\"learning_rate\"].item(), global_step)\n",
        "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/loss\", loss.item(), global_step)\n",
        "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
        "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "        writer.add_scalar(\n",
        "            \"charts/SPS_update\", int(args.num_envs * args.num_steps / (time.time() - update_time_start)), global_step\n",
        "        )\n",
        "\n",
        "    envs.close()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYxfE2Yfifkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}