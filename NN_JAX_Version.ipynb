{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/instadeepai/jumanji.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8J4iqlNbcg5g",
        "outputId": "9abb988c-dba8-4ea1-e576-a9cd5b44b1d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/instadeepai/jumanji.git\n",
            "  Cloning https://github.com/instadeepai/jumanji.git to /tmp/pip-req-build-x71pe6q2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/instadeepai/jumanji.git /tmp/pip-req-build-x71pe6q2\n",
            "  Resolved https://github.com/instadeepai/jumanji.git to commit 10958866909d434ba50edc1915247e4cebc3cb3e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.25.2)\n",
            "Collecting dm-env>=1.5\n",
            "  Downloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Collecting Pillow>=9.0.0\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.2.26 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.3.25)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (4.4.0)\n",
            "Collecting pygame>=2.0.0\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.74 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.3.25+cuda11.cudnn805)\n",
            "Collecting matplotlib>=3.3.4\n",
            "  Downloading matplotlib-3.6.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chex>=0.1.3\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brax>=0.0.10\n",
            "  Downloading brax-0.1.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.3/471.3 KB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (1.21.6)\n",
            "Collecting trimesh==3.9.35\n",
            "  Downloading trimesh-3.9.35-py3-none-any.whl (639 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m639.3/639.3 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (2.11.3)\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.7.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.3.0)\n",
            "Collecting pytinyrenderer\n",
            "  Downloading pytinyrenderer-0.0.13-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxopt\n",
            "  Downloading jaxopt-0.5.5-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco\n",
            "  Downloading mujoco-2.3.1.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flax\n",
            "  Downloading flax-0.6.3-py3-none-any.whl (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.51.1)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from trimesh==3.9.35->brax>=0.0.10->jumanji==0.1.4) (57.4.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.3->jumanji==0.1.4) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.3->jumanji==0.1.4) (0.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (2.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (0.0.8)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.2.26->jumanji==0.1.4) (3.3.0)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (21.3)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.22.0->jumanji==0.1.4) (3.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->jumanji==0.1.4) (1.15.0)\n",
            "Collecting orbax\n",
            "  Downloading orbax-0.1.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.4) (1.0.4)\n",
            "Collecting rich>=11.1\n",
            "  Downloading rich-13.2.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.4) (6.0)\n",
            "Collecting tensorstore\n",
            "  Downloading tensorstore-0.1.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->brax>=0.0.10->jumanji==0.1.4) (2.0.1)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.8/dist-packages (from mujoco->brax>=0.0.10->jumanji==0.1.4) (3.1.6)\n",
            "Collecting glfw\n",
            "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX->brax>=0.0.10->jumanji==0.1.4) (3.19.6)\n",
            "Collecting markdown-it-py<3.0.0,>=2.1.0\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.1->flax->brax>=0.0.10->jumanji==0.1.4) (2.6.1)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.8/dist-packages (from orbax->flax->brax>=0.0.10->jumanji==0.1.4) (1.0.0)\n",
            "Collecting cached_property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from orbax->flax->brax>=0.0.10->jumanji==0.1.4) (3.6.4)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.8/dist-packages (from orbax->flax->brax>=0.0.10->jumanji==0.1.4) (5.10.2)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (9.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (22.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (0.7.1)\n",
            "Building wheels for collected packages: jumanji\n",
            "  Building wheel for jumanji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jumanji: filename=jumanji-0.1.4-py3-none-any.whl size=166864 sha256=a3cca62ae926c72ff223ffc2a3cb37edef7c55b0bbfb522870e58b8247396048\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-19y0lah7/wheels/df/c6/15/caef8b041b929f4f3b7a55a217c00f24c6931fe57ae40d9bd9\n",
            "Successfully built jumanji\n",
            "Installing collected packages: pytinyrenderer, glfw, dataclasses, cached_property, trimesh, tensorstore, tensorboardX, pygame, Pillow, mujoco, mdurl, fonttools, dm-env, contourpy, matplotlib, markdown-it-py, rich, jaxopt, chex, optax, orbax, flax, brax, jumanji\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed Pillow-9.4.0 brax-0.1.1 cached_property-1.5.2 chex-0.1.5 contourpy-1.0.7 dataclasses-0.6 dm-env-1.6 flax-0.6.3 fonttools-4.38.0 glfw-2.5.5 jaxopt-0.5.5 jumanji-0.1.4 markdown-it-py-2.1.0 matplotlib-3.6.3 mdurl-0.1.2 mujoco-2.3.1.post1 optax-0.1.4 orbax-0.1.0 pygame-2.1.2 pytinyrenderer-0.0.13 rich-13.2.0 tensorboardX-2.5.1 tensorstore-0.1.30 trimesh-3.9.35\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install --upgrade jax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIo3tBdkmOqj",
        "outputId": "baaeda6f-36f5-4d8e-c3df-83d6dd53b987"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.8/dist-packages (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import jumanji\n",
        "from jumanji.wrappers import AutoResetWrapper\n"
      ],
      "metadata": {
        "id": "dFa4X-Aqcl34"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(obs):\n",
        "  p = np.concatenate([obs.ems.x1,obs.ems.x2,\n",
        "                       obs.ems.y1,obs.ems.y2,\n",
        "                        obs.ems.z1,obs.ems.z2,\n",
        "                        obs.ems_mask.flatten(),obs.items.x_len,\n",
        "                        obs.items.y_len,obs.items.z_len,\n",
        "                        obs.items_mask.flatten(),obs.items_placed.flatten()])\n",
        "  return p\n"
      ],
      "metadata": {
        "id": "5YYrONQCcrnV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super(FeedForwardNN, self).__init__()\n",
        "    self.layer1 =nn.Linear(in_dim, 64)\n",
        "    self.layer2 =nn.Linear(64, out_dim)\n",
        "    #self.layer3 = nn.Softmax()\n",
        "  def forward(self, obs):\n",
        "  # Convert observation to tensor if it's a numpy array\n",
        "    if isinstance(obs, np.ndarray):\n",
        "      obs = jnp.array(obs, dtype=jnp.float32)\n",
        "  \n",
        "    activation1 = jax.nn.relu(self.layer1(obs))\n",
        "    output = jax.nn.relu(self.layer2(activation1))\n",
        "    print('relu output inside NN:',output)\n",
        "    #output = self.layer3(activation2)\n",
        "    return output"
      ],
      "metadata": {
        "id": "FxO1aLnIcvux"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PPO"
      ],
      "metadata": {
        "id": "LJopZ-Roc1Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import MultivariateNormal,Categorical\n",
        "from torch.optim import Adam \n",
        "import jax.numpy as jnp\n",
        "from jax import nn, random\n",
        "from jax import optimizers\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class PPO:\n",
        "  def __init__(self,env):\n",
        "    self._init_hyperparameters()\n",
        "    self.env = env\n",
        "    #####################################\n",
        "    self.obs_dim = 380\n",
        "    self.act_dim = 800\n",
        "    ######################################\n",
        "    \n",
        "\n",
        "    #initiate actor and critic\n",
        "    self.actor = FeedForwardNN(self.obs_dim,self.act_dim)\n",
        "    self.critic = FeedForwardNN(self.obs_dim,1)\n",
        "\n",
        "    # Create our variable for the matrix.\n",
        "    # Note that I chose 0.5 for stdev arbitrarily.\n",
        "    self.cov_var = jnp.full(shape=(self.act_dim,), fill_value=0.5)\n",
        "    \n",
        "    # Create the covariance matrix\n",
        "    # Create the covariance matrix\n",
        "    self.cov_mat = jnp.diag(self.cov_var)\n",
        "    self.actor_optim = optimizers.Adam(self.actor.parameters(), lr=self.lr)\n",
        "    self.critic_optim = optimizers.Adam(self.critic.parameters(), lr=self.lr)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "  def _init_hyperparameters(self):\n",
        "    # Default values for hyperparameters, will need to change later.\n",
        "    self.timesteps_per_batch = 2  #4800            # timesteps per batch\n",
        "    #self.max_timesteps_per_episode = 1600      # timesteps per episode\n",
        "    self.gamma = 0.95\n",
        "    self.n_updates_per_iteration = 5\n",
        "    self.clip = 0.2 # As recommended by the paper\n",
        "    self.lr = 0.005\n",
        "\n",
        "  def compute_rtgs(self, batch_rews): \n",
        "    # The rewards-to-go (rtg) per episode per batch to return.\n",
        "    # The shape will be (num timesteps per episode)\n",
        "    batch_rtgs = []\n",
        "    #print(\"batch_rewards \",batch_rews)\n",
        "    # Iterate through each episode backwards to maintain same order\n",
        "    # in batch_rtgs\n",
        "    for ep_rews in reversed(batch_rews):\n",
        "      discounted_reward = 0 # The discounted reward so far\n",
        "      for rew in reversed(ep_rews):\n",
        "        discounted_reward = rew + discounted_reward * self.gamma\n",
        "        batch_rtgs.insert(0, discounted_reward)\n",
        "    # Convert the rewards-to-go into a tensor\n",
        "    #print(\"discounted reward of batch\", batch_rtgs)\n",
        "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
        "    return batch_rtgs\n",
        "  \n",
        "    \n",
        "\n",
        "  def get_action(self, obs, action_tensor):\n",
        "    p1 = self.actor(obs)\n",
        "    indices = jnp.nonzero(action_tensor.numpy())\n",
        "    #indices = jnp.nonzero(action_tensor)\n",
        "    p2 = p1[indices]\n",
        "    p2 = jnp.reshape(p2, (-1,))\n",
        "    probs = jax.nn.softmax(p2)\n",
        "    action = jnp.random.choice(jnp.arange(p2.shape[0]), p=probs)\n",
        "    action_id = indices[action].item()\n",
        "    return action_id, action, jnp.log(probs[action])\n",
        "\n",
        "    #  ind = action_mask.index[[action_mask[0] == True]]\n",
        "    #  df = pd.DataFrame(p1.detach().numpy())\n",
        "    #  probs = Categorical(probs=torch.tensor(list(df.iloc[ind][0])))\n",
        "    #  action = probs.sample()\n",
        "    #  action_id = df.iloc[ind][0][action].index\n",
        "     \n",
        "\n",
        "\n",
        "   \n",
        "  def rollout(self):\n",
        "    # Batch data\n",
        "    batch_obs = []             # batch observations\n",
        "    batch_acts = []            # batch actions\n",
        "    batch_log_probs = []       # log probs of each action\n",
        "    batch_rews = []            # batch rewards\n",
        "    batch_rtgs = []            # batch rewards-to-go\n",
        "    batch_lens = []            # episodic lengths in batch\n",
        "                 # for animation\n",
        "    # Number of timesteps run so far this batch´\n",
        "    \n",
        "    step_fn = jax.jit(self.env.step)\n",
        "    reset_fn = jax.jit(self.env.reset)\n",
        "    t = 0 \n",
        "    while t < self.timesteps_per_batch:\n",
        "      # Rewards this episode\n",
        "      ep_rews = []\n",
        "      key = jax.random.PRNGKey(0)\n",
        "      ###############################\n",
        "      #jax.jit(env.reset)(key)\n",
        "      state, timestep = reset_fn(key)\n",
        "     \n",
        "      ###############################\n",
        "      ep_t = 0\n",
        "      rew = 0.0\n",
        "      #for ep_t in range(self.max_timesteps_per_episode):\n",
        "      while rew == 0.0:\n",
        "        # Increment timesteps ran this batch so far\n",
        "        t += 1\n",
        "        # Collect observation\n",
        "        ################################################\n",
        "        obs = flatten(timestep.observation)\n",
        "        #obs = torch.tensor(obs, dtype=torch.float)\n",
        "        batch_obs.append(obs)\n",
        "        \n",
        "        num_ems, num_items = self.env.action_spec().num_values\n",
        "        action_mask = timestep.observation.action_mask.flatten()\n",
        "        action_tensor = torch.tensor(np.array(action_mask),dtype=torch.float)\n",
        "         \n",
        "        \n",
        "        #----------------------------------------------- get from NN\n",
        "        #ems_item_id = self.get_action(obs,action_mask)\n",
        "        ems_item_id, action_,log_prob  = self.get_action(obs,action_tensor)\n",
        "        # -------------------------------------------------\n",
        "        ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
        "\n",
        "        # Wrap the action as a jax array of shape (2,)\n",
        "        action = jnp.array([ems_id, item_id])\n",
        "\n",
        "        #action = torch.tensor(action, dtype=torch.float)\n",
        "        #ems_item_id, action_,log_prob  = self.get_action(obs,action_mask)\n",
        "        #mean = self.actor(obs)\n",
        "        #dist = MultivariateNormal(mean, self.cov_mat)\n",
        "        #log_prob = dist.log_prob(action)\n",
        "        #batch_states.append(state)\n",
        "        state,timestep = step_fn(state, action)\n",
        "        rew = np.array(timestep.reward.flatten())[0]\n",
        "        ##################################################\n",
        "        # Collect reward, action, and log prob\n",
        "        ep_rews.append(rew)\n",
        "        #print(\"reward \", rew)\n",
        "        batch_acts.append(action_)\n",
        "        \n",
        "        print(\"action_  \",action_)\n",
        "        print('flog_prob',log_prob)\n",
        "        batch_log_probs.append(log_prob)\n",
        "        #print(\"log_probs \", log_prob)\n",
        "        ep_t += 1\n",
        "      # Collect episodic length and rewards\n",
        "      #print(\"end of episode\", ep_t)\n",
        "      batch_lens.append(ep_t + 1) # plus 1 because timestep starts at 0\n",
        "      #print(\"episode length  ,\" , ep_t+1)\n",
        "      batch_rews.append(ep_rews) \n",
        "      #print(\"episode rewards \", ep_rews)\n",
        "      # Reshape data as tensors in the shape specified before returning\n",
        "    #print(\"end of batch \")\n",
        "    batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
        "    #print(\"batch_observation \", batch_obs)\n",
        "    batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
        "    #print(\"batch_action \", batch_acts)\n",
        "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
        "    #print(\"batch log_probs \",batch_log_probs)\n",
        "    # ALG STEP #4\n",
        "    #print(\"batch_rewards \",batch_rews)\n",
        "    batch_rtgs = self.compute_rtgs(batch_rews)\n",
        "    #print(\"discounted rewards \",batch_rtgs)\n",
        "    #env.render(state)\n",
        "    # Return the batch data\n",
        "    return batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens,rew\n",
        "\n",
        "  def learn(self, total_timesteps):\n",
        "    t_so_far = 0 # Timesteps simulated so far\n",
        "    episode_reward = []\n",
        "    #print(\"total timesteps \",total_timesteps, \" updates per iteration \", self.n_updates_per_iteration)\n",
        "    while t_so_far < total_timesteps:              # ALG STEP 2\n",
        "      # Increment t_so_far somewhere below\n",
        "      # ALG STEP 3\n",
        "      batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens,rew = self.rollout()\n",
        "      episode_reward.append(rew)\n",
        "      # Calculate how many timesteps we collected this batch   \n",
        "      t_so_far += np.sum(batch_lens)\n",
        "      #print(\"t_so_far \",t_so_far)\n",
        "\n",
        "      # Calculate V_{phi, k}\n",
        "      V, _ = self.evaluate(batch_obs, batch_acts)\n",
        "      #print(\" after evaluate \", V , _)\n",
        "      # ALG STEP 5\n",
        "      # Calculate advantage\n",
        "      A_k = batch_rtgs - V.detach()\n",
        "      # Normalize advantages\n",
        "      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
        "      for i in range(self.n_updates_per_iteration):\n",
        "        # Calculate V_phi and pi_theta(a_t | s_t)    \n",
        "        #print(\" update no per iteration \",i)\n",
        "        V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
        "        # Calculate ratios\n",
        "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
        "        # Calculate surrogate losses\n",
        "        surr1 = ratios * A_k\n",
        "        surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k\n",
        "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
        "        #print(\"actor_loss \", actor_loss)\n",
        "        # Calculate gradients and perform backward propagation for actor \n",
        "        # network\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optim.step()\n",
        "        critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
        "        #print(\"critic_loss \", critic_loss)\n",
        "        # Calculate gradients and perform backward propagation for critic network    \n",
        "        self.critic_optim.zero_grad()    \n",
        "        critic_loss.backward()    \n",
        "        self.critic_optim.step()\n",
        "    return episode_reward\n",
        "    \n",
        "  def evaluate(self, batch_obs,batch_acts):\n",
        "    # Query critic network for a value V for each obs in batch_obs.\n",
        "    V = self.critic(batch_obs).squeeze()\n",
        "    #print(\"in evaluate , value function after critic \", V)\n",
        "    # Calculate the log probabilities of batch actions using most \n",
        "    # recent actor network.\n",
        "    # This segment of code is similar to that in get_action()\n",
        "    mean = self.actor(batch_obs)\n",
        "    #print(\"after actor \", mean)\n",
        "    dist = Categorical(mean)\n",
        "    log_probs = dist.log_prob(batch_acts)\n",
        "    print('log_probs_in_evaluate',log_probs)\n",
        "    # Return predicted values V and log probs log_probs\n",
        "    return V, log_probs\n",
        " \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "-mWtwQvhc0jt",
        "outputId": "f35051ab-998a-46a7-f40b-25a8dbd57d13"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f15335f78697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'optimizers' from 'jax' (/usr/local/lib/python3.8/dist-packages/jax/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "env = jumanji.make(\"BinPack-toy-v0\")\n",
        "model = PPO(env)\n",
        "episode_rewards = model.learn(50)\n",
        "#batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens,batch_states = model.rollout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "p-PJkq6_ddVP",
        "outputId": "98cf633b-6209-421b-a79f-166c1f2cf8c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d5b4e4bca4a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjumanji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BinPack-toy-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#batch_obs, batch_acts,batch_log_probs, batch_rtgs, batch_lens,batch_states = model.rollout()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-287c268c336b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Create the covariance matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: diag(): argument 'input' (position 1) must be Tensor, not Array"
          ]
        }
      ]
    }
  ]
}