{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZVnImURuub2YNg+Gqu0Wy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiamj/deep-reinforcement-learning-jumanji/blob/main/ppo_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = jnp.array([])\n",
        "b2 = jnp.array([2,52,4,7])\n",
        "b = jnp.append(b,b2)\n",
        "jnp.vstack([b,b2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGS90rmFxA_9",
        "outputId": "ea759ec3-3c6f-4628-d164-1770f5cbd80a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[ 2., 52.,  4.,  7.],\n",
              "             [ 2., 52.,  4.,  7.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as np\n",
        "from jax import random,vmap\n",
        "\n",
        "def sum_samples(layer):\n",
        "    (dw, db) = layer\n",
        "    (dw, db) = (np.sum(dw, axis=0), np.sum(db, axis=0))\n",
        "    return np.array([dw, db])\n",
        "\n",
        "key = random.PRNGKey(1701)\n",
        "data = random.uniform(key, (10, 2, 20))\n",
        "\n",
        "result = vmap(sum_samples)(data)\n",
        "print(result.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj-fD9dERrmS",
        "outputId": "d8d6e3ca-3fc0-425a-d6d6-0e0a162d9968"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVXY2__HR2Bt",
        "outputId": "46a7c260-1037-4ed1-8e61-045f517d8933"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[9.10784841e-01, 9.00956511e-01, 4.08365369e-01,\n",
              "               7.35841751e-01, 8.00690055e-01, 5.39383888e-02,\n",
              "               6.56396985e-01, 4.83418584e-01, 9.21171427e-01,\n",
              "               8.80449653e-01, 3.09709907e-01, 7.77801037e-01,\n",
              "               8.92477393e-01, 9.61626410e-01, 9.99255180e-01,\n",
              "               1.37289643e-01, 6.37572408e-01, 5.11163592e-01,\n",
              "               4.69839931e-01, 9.16154027e-01],\n",
              "              [8.52572918e-01, 7.37004638e-01, 1.45462155e-01,\n",
              "               4.26606178e-01, 8.60655069e-01, 4.40370917e-01,\n",
              "               4.21858072e-01, 2.93413281e-01, 1.59504175e-01,\n",
              "               6.33872390e-01, 5.46415687e-01, 6.35575294e-01,\n",
              "               8.22544098e-06, 7.14041829e-01, 3.95977378e-01,\n",
              "               5.39646029e-01, 8.98891687e-02, 4.99658227e-01,\n",
              "               2.09191322e-01, 5.51786423e-02]],\n",
              "\n",
              "             [[3.44063640e-01, 7.59088278e-01, 8.75769258e-01,\n",
              "               2.05692530e-01, 7.68872142e-01, 4.85912561e-02,\n",
              "               3.76576543e-01, 1.59282923e-01, 6.66122317e-01,\n",
              "               7.68826008e-02, 7.47599483e-01, 4.84158754e-01,\n",
              "               1.13964438e-01, 1.42050505e-01, 5.98236561e-01,\n",
              "               2.87550688e-01, 5.98336101e-01, 6.72990084e-01,\n",
              "               6.53721213e-01, 9.38372016e-01],\n",
              "              [7.06639409e-01, 4.39164758e-01, 6.90080285e-01,\n",
              "               3.29041600e-01, 8.50655437e-01, 5.29577136e-01,\n",
              "               9.46985483e-02, 9.70868230e-01, 6.78333998e-01,\n",
              "               9.48850155e-01, 1.83965802e-01, 3.29248190e-01,\n",
              "               5.87361813e-01, 1.05456710e-01, 3.62094283e-01,\n",
              "               2.96609163e-01, 6.69184089e-01, 7.38373280e-01,\n",
              "               7.08543897e-01, 5.80912709e-01]],\n",
              "\n",
              "             [[7.53916025e-01, 6.01453424e-01, 9.61362123e-02,\n",
              "               3.91509533e-02, 8.38008046e-01, 3.42855811e-01,\n",
              "               1.39293432e-01, 7.25058198e-01, 1.85465813e-02,\n",
              "               3.73671174e-01, 3.19508672e-01, 5.89853883e-01,\n",
              "               2.29132295e-01, 8.14817071e-01, 4.01984572e-01,\n",
              "               1.00419760e-01, 8.46624374e-02, 2.55802989e-01,\n",
              "               3.62974286e-01, 1.98571444e-01],\n",
              "              [7.64784455e-01, 5.54041386e-01, 3.31485033e-01,\n",
              "               4.54690218e-01, 2.15105534e-01, 4.58979487e-01,\n",
              "               3.99899483e-02, 4.10602689e-01, 6.76079154e-01,\n",
              "               5.63951492e-01, 1.42629862e-01, 3.13669801e-01,\n",
              "               4.68075275e-01, 7.68883944e-01, 7.19795346e-01,\n",
              "               5.09566784e-01, 9.53910947e-01, 8.93937349e-02,\n",
              "               7.68657446e-01, 5.94772935e-01]],\n",
              "\n",
              "             [[2.50203609e-02, 9.04143810e-01, 3.63489151e-01,\n",
              "               1.46851540e-02, 9.49601531e-01, 7.04073429e-01,\n",
              "               1.94252133e-01, 7.69754529e-01, 9.94401336e-01,\n",
              "               6.00893617e-01, 1.63211823e-02, 2.03214884e-01,\n",
              "               3.59423995e-01, 3.59955907e-01, 8.77693176e-01,\n",
              "               1.21368289e-01, 4.93526459e-03, 4.09233093e-01,\n",
              "               5.13746858e-01, 5.28852940e-01],\n",
              "              [4.25690413e-02, 3.62619162e-02, 6.80603623e-01,\n",
              "               5.21394730e-01, 7.13649631e-01, 8.67375970e-01,\n",
              "               4.65170145e-02, 8.64141703e-01, 2.53773928e-01,\n",
              "               5.09130359e-01, 7.22360134e-01, 6.89697981e-01,\n",
              "               5.51480293e-01, 3.06105971e-01, 6.55730486e-01,\n",
              "               1.43139482e-01, 2.35949636e-01, 2.43965864e-01,\n",
              "               6.24054074e-01, 3.37000847e-01]],\n",
              "\n",
              "             [[3.09192061e-01, 5.72095156e-01, 9.98410344e-01,\n",
              "               3.03847909e-01, 6.50588870e-01, 6.78243041e-01,\n",
              "               6.56285524e-01, 6.49011731e-01, 1.64618731e-01,\n",
              "               8.32495809e-01, 2.34570742e-01, 8.78357887e-02,\n",
              "               6.81891799e-01, 8.89623046e-01, 7.08889127e-01,\n",
              "               6.27156496e-02, 5.34145951e-01, 3.43765020e-01,\n",
              "               7.07569003e-01, 3.49135756e-01],\n",
              "              [3.52290154e-01, 9.02613282e-01, 3.83831263e-02,\n",
              "               1.98168755e-01, 5.91781855e-01, 4.61029887e-01,\n",
              "               6.43818974e-01, 7.88012624e-01, 8.03813338e-01,\n",
              "               7.28622556e-01, 8.55121970e-01, 8.64034057e-01,\n",
              "               2.62810111e-01, 4.82339025e-01, 5.86860299e-01,\n",
              "               9.68924761e-02, 9.08169627e-01, 5.16362548e-01,\n",
              "               7.96468616e-01, 7.61595488e-01]],\n",
              "\n",
              "             [[6.13742828e-01, 5.24129152e-01, 7.87760019e-02,\n",
              "               6.41989946e-01, 8.90257955e-01, 4.91906643e-01,\n",
              "               5.00184536e-01, 8.53640437e-01, 7.96164632e-01,\n",
              "               8.14259171e-01, 7.70532012e-01, 1.95277214e-01,\n",
              "               5.29714584e-01, 6.12041831e-01, 6.59668326e-01,\n",
              "               5.92953324e-01, 4.94185448e-01, 8.92581582e-01,\n",
              "               4.38647747e-01, 2.61825442e-01],\n",
              "              [8.77841830e-01, 4.72866654e-01, 9.89995837e-01,\n",
              "               5.12926459e-01, 5.95345497e-02, 1.02913260e-01,\n",
              "               2.88887024e-02, 8.34637642e-01, 4.39594984e-02,\n",
              "               7.97374249e-01, 2.25934148e-01, 9.04292107e-01,\n",
              "               1.87608004e-02, 5.15744925e-01, 8.20031881e-01,\n",
              "               4.34284329e-01, 7.10308790e-01, 1.79128289e-01,\n",
              "               9.15482640e-01, 6.17255330e-01]],\n",
              "\n",
              "             [[3.18474889e-01, 8.17909956e-01, 2.12130785e-01,\n",
              "               4.42916989e-01, 9.25580621e-01, 2.20725894e-01,\n",
              "               3.75087261e-02, 9.86585140e-01, 6.68431640e-01,\n",
              "               5.82841516e-01, 4.34761763e-01, 6.56974673e-01,\n",
              "               1.24411106e-01, 3.84944797e-01, 5.32364726e-01,\n",
              "               1.81761861e-01, 3.10273409e-01, 6.88444257e-01,\n",
              "               6.10430241e-02, 1.09232664e-01],\n",
              "              [1.41311049e-01, 3.48029971e-01, 1.65927172e-01,\n",
              "               2.66794205e-01, 6.06203794e-01, 7.96128511e-02,\n",
              "               9.28797722e-02, 3.11982393e-01, 7.61072636e-02,\n",
              "               9.52159166e-01, 2.26835132e-01, 1.92552209e-01,\n",
              "               7.57819414e-02, 6.87696457e-01, 8.71516824e-01,\n",
              "               3.12282443e-01, 2.47756958e-01, 6.30819321e-01,\n",
              "               5.49173236e-01, 9.85391736e-01]],\n",
              "\n",
              "             [[1.33571148e-01, 7.54435301e-01, 2.32061028e-01,\n",
              "               3.44546437e-01, 7.43454337e-01, 2.51678109e-01,\n",
              "               9.44656968e-01, 2.36420631e-01, 6.22006059e-01,\n",
              "               8.43002915e-01, 3.68518829e-01, 7.94597149e-01,\n",
              "               1.03322148e-01, 1.29909754e-01, 2.81952500e-01,\n",
              "               7.18468428e-01, 7.29023576e-01, 1.19452238e-01,\n",
              "               6.32119179e-01, 1.18969083e-01],\n",
              "              [9.61788893e-01, 2.50545621e-01, 1.07843757e-01,\n",
              "               5.22949457e-01, 5.20487189e-01, 4.77428913e-01,\n",
              "               4.13414836e-01, 7.91862845e-01, 4.46998239e-01,\n",
              "               3.03465962e-01, 5.91655850e-01, 5.45536876e-01,\n",
              "               5.68796396e-01, 2.74085283e-01, 9.54789042e-01,\n",
              "               1.13472700e-01, 8.69383574e-01, 2.32882500e-01,\n",
              "               3.06130648e-01, 1.57053947e-01]],\n",
              "\n",
              "             [[9.17449594e-01, 2.67053723e-01, 1.88641429e-01,\n",
              "               1.33090496e-01, 7.63668895e-01, 3.12337756e-01,\n",
              "               8.92650485e-01, 9.29988503e-01, 8.60538244e-01,\n",
              "               4.57568169e-01, 4.61078525e-01, 2.48217821e-01,\n",
              "               2.27879643e-01, 3.23737025e-01, 8.91143084e-02,\n",
              "               4.76294756e-01, 4.28268313e-01, 6.79367542e-01,\n",
              "               5.97333312e-01, 3.61006737e-01],\n",
              "              [8.96548867e-01, 2.93034434e-01, 5.23700714e-02,\n",
              "               4.66383696e-02, 3.45339060e-01, 9.90470529e-01,\n",
              "               9.67285752e-01, 6.36311889e-01, 1.71997309e-01,\n",
              "               3.98589373e-02, 3.66045475e-01, 5.50078869e-01,\n",
              "               4.95531440e-01, 4.51392055e-01, 2.61353612e-01,\n",
              "               6.71226740e-01, 3.02927494e-01, 8.34289432e-01,\n",
              "               2.21775413e-01, 4.84277248e-01]],\n",
              "\n",
              "             [[3.73022079e-01, 2.46072412e-01, 3.58117104e-01,\n",
              "               4.27853465e-01, 6.73961163e-01, 9.27092791e-01,\n",
              "               3.45356345e-01, 5.90945125e-01, 7.55282640e-02,\n",
              "               9.46051955e-01, 7.25440502e-01, 2.13555098e-02,\n",
              "               9.63211536e-01, 6.59143925e-03, 9.61769819e-02,\n",
              "               1.57614112e-01, 8.63839269e-01, 7.40685463e-02,\n",
              "               1.54764175e-01, 1.27662539e-01],\n",
              "              [5.91229796e-01, 2.14100122e-01, 1.67573810e-01,\n",
              "               1.92027688e-01, 3.87608886e-01, 1.37943029e-02,\n",
              "               5.75331807e-01, 2.38584280e-01, 5.91475010e-01,\n",
              "               2.07999349e-01, 4.72309828e-01, 2.18504190e-01,\n",
              "               5.62216043e-01, 5.98111868e-01, 3.50712419e-01,\n",
              "               9.32108164e-01, 8.96690845e-01, 9.49527025e-01,\n",
              "               6.21761680e-01, 8.88010502e-01]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.append([2],[3],axis = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4LV3zSbMB8P",
        "outputId": "06a962a8-f8e2-48be-a521-f59641a324b4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.concatenate([b,b2],axis =1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "TrBvhuY5HaeE",
        "outputId": "3ee50260-b114-4951-95f0-2439f0e079d7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b6ed23dea3e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(arrays, axis, dtype)\u001b[0m\n\u001b[1;32m   1779\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_canonicalize_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m     \u001b[0marrays_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_promote_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mcanonicalize_axis\u001b[0;34m(axis, num_dims)\u001b[0m\n\u001b[1;32m    349\u001b[0m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnum_dims\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"axis {axis} is out of bounds for array of dimension {num_dims}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = [2,3 ,4 ]\n",
        "c.append([3,45])"
      ],
      "metadata": {
        "id": "xIIgzoB2Hx8D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYPtDuNAH43v",
        "outputId": "213809db-4046-4a07-e52f-55ed8ae0cc99"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, [3, 45]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/instadeepai/jumanji.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMWu06-soreA",
        "outputId": "5040ee00-fe9a-49a4-99fd-652820f0abe1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/instadeepai/jumanji.git\n",
            "  Cloning https://github.com/instadeepai/jumanji.git to /tmp/pip-req-build-ptl5smkc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/instadeepai/jumanji.git /tmp/pip-req-build-ptl5smkc\n",
            "  Resolved https://github.com/instadeepai/jumanji.git to commit 10958866909d434ba50edc1915247e4cebc3cb3e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dm-env>=1.5\n",
            "  Downloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: jax>=0.2.26 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.3.25)\n",
            "Collecting Pillow>=9.0.0\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (4.4.0)\n",
            "Requirement already satisfied: gym>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (1.21.6)\n",
            "Requirement already satisfied: jaxlib>=0.1.74 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.3.25+cuda11.cudnn805)\n",
            "Collecting chex>=0.1.3\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygame>=2.0.0\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brax>=0.0.10\n",
            "  Downloading brax-0.1.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.3/471.3 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib>=3.3.4\n",
            "  Downloading matplotlib-3.6.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxopt\n",
            "  Downloading jaxopt-0.5.5-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flax\n",
            "  Downloading flax-0.6.4-py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytinyrenderer\n",
            "  Downloading pytinyrenderer-0.0.13-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.3.0)\n",
            "Collecting trimesh==3.9.35\n",
            "  Downloading trimesh-3.9.35-py3-none-any.whl (639 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m639.3/639.3 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco\n",
            "  Downloading mujoco-2.3.1.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.7.3)\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (2.11.3)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.51.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from trimesh==3.9.35->brax>=0.0.10->jumanji==0.1.4) (57.4.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.3->jumanji==0.1.4) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.3->jumanji==0.1.4) (0.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.2.26->jumanji==0.1.4) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (21.3)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.22.0->jumanji==0.1.4) (3.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->jumanji==0.1.4) (1.15.0)\n",
            "Collecting rich>=11.1\n",
            "  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.4) (6.0)\n",
            "Collecting orbax\n",
            "  Downloading orbax-0.1.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorstore\n",
            "  Downloading tensorstore-0.1.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.4) (1.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->brax>=0.0.10->jumanji==0.1.4) (2.0.1)\n",
            "Collecting glfw\n",
            "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.8/dist-packages (from mujoco->brax>=0.0.10->jumanji==0.1.4) (3.1.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX->brax>=0.0.10->jumanji==0.1.4) (3.19.6)\n",
            "Collecting markdown-it-py<3.0.0,>=2.1.0\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.14.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_resources in /usr/local/lib/python3.8/dist-packages (from orbax->flax->brax>=0.0.10->jumanji==0.1.4) (5.10.2)\n",
            "Collecting cached_property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.8/dist-packages (from orbax->flax->brax>=0.0.10->jumanji==0.1.4) (1.0.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: jumanji\n",
            "  Building wheel for jumanji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jumanji: filename=jumanji-0.1.4-py3-none-any.whl size=166864 sha256=82f737b5bd44c2cb977b02526a9ce949d34f4ff391c9e1f04871db3392353d8b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xpzrv5cn/wheels/df/c6/15/caef8b041b929f4f3b7a55a217c00f24c6931fe57ae40d9bd9\n",
            "Successfully built jumanji\n",
            "Installing collected packages: pytinyrenderer, glfw, dataclasses, cached_property, trimesh, tensorstore, tensorboardX, pygments, pygame, Pillow, mujoco, mdurl, fonttools, dm-env, contourpy, matplotlib, markdown-it-py, rich, jaxopt, chex, optax, orbax, flax, brax, jumanji\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.4.0 brax-0.1.1 cached_property-1.5.2 chex-0.1.5 contourpy-1.0.7 dataclasses-0.6 dm-env-1.6 flax-0.6.4 fonttools-4.38.0 glfw-2.5.5 jaxopt-0.5.5 jumanji-0.1.4 markdown-it-py-2.1.0 matplotlib-3.6.3 mdurl-0.1.2 mujoco-2.3.1.post1 optax-0.1.4 orbax-0.1.1 pygame-2.1.2 pygments-2.14.0 pytinyrenderer-0.0.13 rich-13.3.1 tensorboardX-2.5.1 tensorstore-0.1.30 trimesh-3.9.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upuzSlMOHB8T",
        "outputId": "5087a133-cb4a-42dc-94d4-e76937a98865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jumanji\n",
        "from jumanji.wrappers import AutoResetWrapper\n",
        "import flax.linen as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def flatten_jax(obs):\n",
        "    return jnp.concatenate([obs.ems.x1,obs.ems.x2,\n",
        "                        obs.ems.y1,obs.ems.y2,\n",
        "                        obs.ems.z1,obs.ems.z2,\n",
        "                        obs.ems_mask.flatten(),obs.items.x_len,\n",
        "                        obs.items.y_len,obs.items.z_len,\n",
        "                        obs.items_mask.flatten(),obs.items_placed.flatten()])"
      ],
      "metadata": {
        "id": "Ya00mTGWHGBJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn \n",
        "import optax\n",
        "\n",
        "class SimpleClassifierCompact(nn.Module):\n",
        "    num_hidden : int   # Number of hidden neurons\n",
        "    num_outputs : int  # Number of output neurons\n",
        "\n",
        "    @nn.compact  # Tells Flax to look for defined submodules\n",
        "    def __call__(self, x):\n",
        "        # Perform the calculation of the model to determine the prediction\n",
        "        # while defining necessary layers\n",
        "        x = nn.Dense(features=self.num_hidden)(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dense(features=self.num_outputs)(x)\n",
        "        return x\n",
        "def critic_calculate_loss( V,batch_rtgs):\n",
        "    #logits = critic_state.apply_fn(params, data).squeeze(axis=-1)\n",
        "    loss = optax.sigmoid_binary_cross_entropy(V, batch_rtgs).mean()\n",
        "    return loss\n",
        "\n",
        "@jax.jit  # Jit the function for efficiency\n",
        "def critic_train_step(state, V,batch_rtgs):\n",
        "    # Gradient function\n",
        "    grad_fn = jax.value_and_grad(critic_calculate_loss,  # Function to calculate the loss\n",
        "                                 argnums=1  # Parameters are second argument of the function\n",
        "                                 #has_aux=False  # Function has additional outputs, here accuracy\n",
        "                                )\n",
        "    # Determine gradients for current model, parameters and batch\n",
        "    loss, grads = grad_fn(V,batch_rtgs)\n",
        "    # Perform parameter update with gradients and optimizer\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    # Return state and any other value we might want\n",
        "    return state\n",
        "def actor_calculate_loss( surr1,surr2):\n",
        "    #logits = critic_state.apply_fn(params, data).squeeze(axis=-1)\n",
        "    actor_loss = (-jnp.min(surr1, surr2)).mean()\n",
        "    return actor_loss\n",
        "\n",
        "@jax.jit  # Jit the function for efficiency\n",
        "def actor_train_step(state,  surr1,surr2):\n",
        "    # Gradient function\n",
        "    grad_fn = jax.value_and_grad(actor_calculate_loss,  # Function to calculate the loss\n",
        "                                 argnums=1  # Parameters are second argument of the function\n",
        "                                 #has_aux=False  # Function has additional outputs, here accuracy\n",
        "                                )\n",
        "    # Determine gradients for current model, parameters and batch\n",
        "    loss, grads = grad_fn( surr1,surr2)\n",
        "    # Perform parameter update with gradients and optimizer\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    # Return state and any other value we might want\n",
        "    return state\n",
        "#critic_state, loss = critic_train_step(critic_state, data)"
      ],
      "metadata": {
        "id": "XOsEpJk7HMjx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training import train_state\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "class PPO:\n",
        "  def __init__(self,env):\n",
        "    self._init_hyperparameters()\n",
        "    self.env = env\n",
        "    \n",
        "    #initiate actor and critic\n",
        "    \n",
        "\n",
        "    self.optimizer = optax.sgd(learning_rate=self.lr)\n",
        "\n",
        "    self.actor = SimpleClassifierCompact(num_hidden=64, num_outputs=self.act_dim)\n",
        "    self.critic = SimpleClassifierCompact(num_hidden=64, num_outputs=1)\n",
        "    \n",
        "    self.params = self.actor.init(self.subkey, jnp.arange(self.obs_dim))\n",
        "\n",
        "    self.actor_state = train_state.TrainState.create(apply_fn=self.actor.apply,\n",
        "                                            params=self.params,\n",
        "                                            tx=self.optimizer)\n",
        "    self.critic_state = train_state.TrainState.create(apply_fn=self.critic.apply,\n",
        "                                            params=self.params,\n",
        "                                            tx=self.optimizer)\n",
        "     \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  def _init_hyperparameters(self):\n",
        "    # Default values for hyperparameters, will need to change later.\n",
        "    self.timesteps_per_batch = 2  #4800            # timesteps per batch\n",
        "    #self.max_timesteps_per_episode = 1600      # timesteps per episode\n",
        "    self.gamma = 0.95\n",
        "    self.n_updates_per_iteration = 1\n",
        "    self.clip = 0.2 # As recommended by the paper\n",
        "    self.lr = 0.1\n",
        "\n",
        "    self.obs_dim = 380\n",
        "    self.act_dim = 800\n",
        "\n",
        "    self.key = jax.random.PRNGKey(0)\n",
        "    self.key, self.subkey = jax.random.split(self.key)\n",
        "\n",
        "\n",
        "  def compute_rtgs_jax(self, batch_rews): \n",
        "    # The rewards-to-go (rtg) per episode per batch to return.\n",
        "    # The shape will be (num timesteps per episode)\n",
        "    \n",
        "    batch_rtgs = jnp.array([])\n",
        "    print(\"batch_rews \",batch_rews)\n",
        "    # Iterate through each episode backwards to maintain same order in batch_rtgs\n",
        "    for ep_rews in reversed(batch_rews):\n",
        "      discounted_reward = 0 # The discounted reward so far\n",
        "      print(\" ep_rews \",ep_rews)\n",
        "      discounted_reward = ep_rews + discounted_reward * self.gamma\n",
        "      print(\"discounted_reward \",discounted_reward)\n",
        "      batch_rtgs = jnp.insert(batch_rtgs, 0, discounted_reward)\n",
        "    print(\"batch_rtgs\",batch_rtgs)\n",
        "    return batch_rtgs\n",
        "\n",
        "\n",
        " \n",
        "  def get_action_jax(self, obs, action_jnp):\n",
        "    #flatten observation  p = flatten(timestep.observation)\n",
        "    # inside critic and actor by converting it to np.array(p) by actor you will get action\n",
        "\n",
        "    #initiate actor and critic\n",
        "     #actor = FeedForwardNN(380,800)\n",
        "     logits = self.actor_state.apply_fn(self.actor_state.params, obs)\n",
        "    \n",
        "     valid_indices = jnp.nonzero(action_jnp)  # getting valid indicies\n",
        "     valid_logits = logits[valid_indices]  # getting proper valid action probablities\n",
        "     valid_logits_array = jnp.reshape(valid_logits, (-1,))  # reshapping it to make it 1D\n",
        "     index_mapping = [i for i, include in enumerate(action_jnp) if include] # mapping of valid actions on whole set of actions\n",
        "     # Gumbel's trick\n",
        "     \n",
        "     u = jax.random.uniform(self.subkey, shape=valid_logits_array.shape) # generates random uniform values\n",
        "     \n",
        "     probs = valid_logits_array - jnp.log(-jnp.log(u)) # logits + random uniform noise\n",
        "     action = jnp.argmax(probs) # argmax of probs -> action id in filtered array from valid actions\n",
        "     action_id = index_mapping[action] # action index in the 800 size array from actor output\n",
        "     log_prob_action = jnp.log(action) # log probability of selected action\n",
        "     \n",
        "     return action_id, action, log_prob_action\n",
        "  \n",
        "  def rollout(self):\n",
        "    # batch observations, batch actions, log probs of each action, batch rewards,batch rewards-to-go,episodic lengths in batch\n",
        "    batch_obs, batch_acts, batch_log_probs, batch_rews, batch_rtgs, batch_lens,batch_states = jnp.array([]), jnp.array([]), jnp.array([]), jnp.array([]), jnp.array([]), jnp.array([]),jnp.array([])\n",
        "    \n",
        "    step_fn = jax.jit(self.env.step)\n",
        "    reset_fn = jax.jit(self.env.reset)\n",
        "    t = 0 \n",
        "    \n",
        "    while t < self.timesteps_per_batch: # Number of timesteps run so far this batch\n",
        "      # Rewards this episode\n",
        "      ep_rews = jnp.array([])\n",
        "      \n",
        "      \n",
        "      state, timestep = reset_fn(self.key)\n",
        "      ep_t = 1\n",
        "      rew = 0.0\n",
        "      while rew == 0.0:\n",
        "        \n",
        "        obs = flatten_jax(timestep.observation)  # Collect observation\n",
        "        print(\"obs  flat \",len(obs))\n",
        "        if t == 0 and ep_t == 1:\n",
        "          batch_obs = jnp.append(batch_obs, obs)\n",
        "        else:\n",
        "          batch_obs = jnp.vstack([batch_obs, obs])\n",
        "        print(\"batch  \",batch_obs)\n",
        "        num_ems, num_items = self.env.action_spec().num_values\n",
        "        action_mask = timestep.observation.action_mask.flatten()\n",
        "        action_jnp = jnp.array(action_mask, dtype=jnp.float32)\n",
        "\n",
        "        ems_item_id, action_,log_prob  = self.get_action_jax(obs,action_jnp)\n",
        "        ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
        "\n",
        "        action = jnp.array([ems_id, item_id])  # Wrap the action as a jax array of shape (2,)\n",
        "        #batch_states = jnp.append(batch_states, state)\n",
        "\n",
        "        state,timestep = step_fn(state, action)\n",
        "        rew = jnp.array(timestep.reward.flatten())[0]\n",
        "        print(\" rew \", rew,\" type \", type(rew))\n",
        "        ep_rews = jnp.append(ep_rews, rew)\n",
        "        print(\" ep_rews \",ep_rews )\n",
        "        batch_acts = jnp.append(batch_acts, action_)\n",
        "        batch_log_probs = jnp.append(batch_log_probs, log_prob)\n",
        "        ep_t += 1 # Increment timesteps ran this batch so far\n",
        "\n",
        "      t += ep_t\n",
        "      batch_rews = jnp.append(batch_rews, ep_rews) \n",
        "    \n",
        "    batch_rtgs = self.compute_rtgs_jax(batch_rews)\n",
        "    return batch_obs, batch_acts,batch_log_probs, batch_rtgs, t ,rew\n",
        "\n",
        "  def learn(self, total_timesteps):\n",
        "    t_so_far = 0 # Timesteps simulated so far\n",
        "    episode_reward = jnp.array([])\n",
        "    while t_so_far < total_timesteps:              # ALG STEP \n",
        "      batch_obs, batch_acts, batch_log_probs, batch_rtgs, t, rew = self.rollout()\n",
        "      print(\"len  of batch obs and obs of episode itself\",len(batch_obs),batch_obs)\n",
        "      print(\"batch_ act \", batch_acts)\n",
        "      print(\"batch_log_prob \",batch_log_probs)\n",
        "      print(\"batch_rtg \", batch_rtgs)\n",
        "      \n",
        "      episode_reward = jnp.append(episode_reward, rew)\n",
        "      t_so_far += t # Calculate how many timesteps we collected this batch   \n",
        "      V, _ = self.evaluate(batch_obs, batch_acts)\n",
        "      A_k = batch_rtgs - V # ALG STEP 5 Calculate advantage\n",
        "      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) # Normalize advantages\n",
        "      for i in range(self.n_updates_per_iteration):\n",
        "        V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
        "        ratios = jax.lax.exp(curr_log_probs - batch_log_probs)   # Calculate ratios\n",
        "        surr1 = ratios * A_k  # Calculate surrogate losses\n",
        "        surr2 = jax.lax.clamp( 1 - self.clip, ratios, 1 + self.clip) * A_k\n",
        "        self.actor_state = actor_train_step(self.actor_state, surr1, surr2)\n",
        "        \n",
        "        self.critic_state = critic_train_step(self.critic_state, V, batch_rtgs)\n",
        "        \n",
        "    return episode_reward\n",
        "    \n",
        "  def evaluate(self, batch_obs,batch_acts):\n",
        "    # Query critic network for a value V for each obs in batch_obs.\n",
        "    #V = self.critic(batch_obs).squeeze()\n",
        "\n",
        "    V = self.critic_state.apply_fn(self.critic_state.params, batch_obs).squeeze(axis=-1)\n",
        "\n",
        "    # Calculate the log probabilities of batch actions using most \n",
        "    # recent actor network.  # This segment of code is similar to that in get_action()\n",
        "    batch_logits = self.actor_state.apply_fn(self.actor_state.params, batch_obs).squeeze(axis=-1)\n",
        "    # rescaling \n",
        "    log_softmax_probs = jax.nn.log_softmax(batch_logits)\n",
        "    log_probs = log_softmax_probs[batch_acts]\n",
        "\n",
        "    return V, log_probs  # Return predicted values V and log probs log_probs\n",
        " \n",
        "  "
      ],
      "metadata": {
        "id": "UjzPa8LFHNvL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env = jumanji.make(\"BinPack-toy-v0\")\n",
        "model = PPO(env)\n",
        "#batch_obs, batch_acts, batch_log_probs, batch_rtgs, t, rew = model.rollout()\n",
        "  \n",
        "%time batch_states, episode_rewards = model.learn(1)\n",
        "#### with jit env , reset one episode \n",
        "# CPU times: user 7.4 s, sys: 103 ms, total: 7.5 s\n",
        "# Wall time: 7.1 s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x-37ugMco8ka",
        "outputId": "58165a58-8abf-44e4-da19-480069e566f5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obs  flat  380\n",
            "batch   [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         1.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 1.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         1.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 1.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.4165247  0.52521294\n",
            " 0.3321976  0.5834753  0.52521294 0.47478706 0.5834753  0.47478706\n",
            " 0.47478706 0.5834753  0.52521294 0.47478706 0.22061329 0.47478706\n",
            " 0.52521294 0.5834753  0.5834753  0.14258943 0.19591141 0.5834753\n",
            " 0.56051505 0.61330473 0.5583691  0.13776824 0.5        1.\n",
            " 0.1248927  0.64549357 0.35450643 0.2        0.38669527 0.4416309\n",
            " 0.43948498 1.         0.5        0.2        0.28454936 0.5583691\n",
            " 0.43948498 0.2527897  0.46454546 0.24954545 0.3181818  0.46454546\n",
            " 0.2859091  0.09090909 0.46454546 0.07136364 0.07136364 0.165\n",
            " 0.24954545 0.3181818  0.46454546 0.055      0.2859091  0.29954547\n",
            " 0.46454546 0.3181818  0.46454546 0.46454546 1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.4165247  0.52521294\n",
            "  0.3321976  0.5834753  0.52521294 0.47478706 0.5834753  0.47478706\n",
            "  0.47478706 0.5834753  0.52521294 0.47478706 0.22061329 0.47478706\n",
            "  0.52521294 0.5834753  0.5834753  0.14258943 0.19591141 0.5834753\n",
            "  0.56051505 0.61330473 0.5583691  0.13776824 0.5        1.\n",
            "  0.1248927  0.64549357 0.35450643 0.2        0.38669527 0.4416309\n",
            "  0.43948498 1.         0.5        0.2        0.28454936 0.5583691\n",
            "  0.43948498 0.2527897  0.46454546 0.24954545 0.3181818  0.46454546\n",
            "  0.2859091  0.09090909 0.46454546 0.07136364 0.07136364 0.165\n",
            "  0.24954545 0.3181818  0.46454546 0.055      0.2859091  0.29954547\n",
            "  0.46454546 0.3181818  0.46454546 0.46454546 1.         1.\n",
            "  1.         1.         1.         1.         1.         1.\n",
            "  1.         1.         1.         1.         1.         1.\n",
            "  1.         1.         1.         1.         1.         1.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.52521294 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         1.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.5        0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         1.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.2859091  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         1.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         1.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.4165247  0.52521294\n",
            "  0.3321976  0.5834753  0.52521294 0.47478706 0.5834753  0.47478706\n",
            "  0.47478706 0.5834753  0.52521294 0.47478706 0.22061329 0.47478706\n",
            "  0.52521294 0.5834753  0.5834753  0.14258943 0.19591141 0.5834753\n",
            "  0.56051505 0.61330473 0.5583691  0.13776824 0.5        1.\n",
            "  0.1248927  0.64549357 0.35450643 0.2        0.38669527 0.4416309\n",
            "  0.43948498 1.         0.5        0.2        0.28454936 0.5583691\n",
            "  0.43948498 0.2527897  0.46454546 0.24954545 0.3181818  0.46454546\n",
            "  0.2859091  0.09090909 0.46454546 0.07136364 0.07136364 0.165\n",
            "  0.24954545 0.3181818  0.46454546 0.055      0.2859091  0.29954547\n",
            "  0.46454546 0.3181818  0.46454546 0.46454546 1.         1.\n",
            "  1.         1.         1.         1.         1.         1.\n",
            "  1.         1.         1.         1.         1.         1.\n",
            "  1.         1.         1.         1.         1.         1.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.5834753  ... 0.         0.         1.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.5834753  ... 0.         0.         1.        ]\n",
            " [0.         0.         0.5834753  ... 0.         0.         1.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.5834753  ... 0.         0.         1.        ]\n",
            " [0.         0.         0.5834753  ... 0.         0.         1.        ]\n",
            " [0.         0.5834753  0.         ... 0.         0.         1.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.5834753  ... 0.         0.         1.        ]\n",
            " [0.         0.5834753  0.         ... 0.         0.         1.        ]\n",
            " [0.         0.5834753  0.5834753  ... 0.         0.         1.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.5834753  0.         ... 0.         0.         1.        ]\n",
            " [0.         0.5834753  0.5834753  ... 0.         0.         1.        ]\n",
            " [0.5834753  0.5834753  0.         ... 0.         0.         1.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.5834753  0.5834753  ... 0.         0.         1.        ]\n",
            " [0.5834753  0.5834753  0.         ... 0.         0.         1.        ]\n",
            " [0.5834753  0.7793867  0.5834753  ... 0.         1.         1.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.5834753  0.5834753  0.         ... 0.         0.         1.        ]\n",
            " [0.5834753  0.7793867  0.5834753  ... 0.         1.         1.        ]\n",
            " [0.5834753  0.         0.         ... 1.         1.         1.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.5834753  0.7793867  0.5834753  ... 0.         1.         1.        ]\n",
            " [0.5834753  0.         0.         ... 1.         1.         1.        ]\n",
            " [0.5834753  0.         0.5834753  ... 1.         1.         1.        ]]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "obs  flat  380\n",
            "batch   [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.5834753  0.         0.         ... 1.         1.         1.        ]\n",
            " [0.5834753  0.         0.5834753  ... 1.         1.         1.        ]\n",
            " [0.         0.         0.5834753  ... 1.         1.         1.        ]]\n",
            " rew  0.6203638  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.6203638]\n",
            "batch_rews  [0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.6203638]\n",
            " ep_rews  0.6203638\n",
            "discounted_reward  0.6203638\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.0\n",
            "batch_rtgs [0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.6203638]\n",
            "len  of batch obs and obs of episode itself 14 [[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.52521294 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.5834753  0.         0.         ... 1.         1.         1.        ]\n",
            " [0.5834753  0.         0.5834753  ... 1.         1.         1.        ]\n",
            " [0.         0.         0.5834753  ... 1.         1.         1.        ]]\n",
            "batch_ act  [ 4. 36.  7. 18. 15. 30. 36. 15. 12.  8.  5.  3.  0.  0.]\n",
            "batch_log_prob  [1.3862944 3.583519  1.9459102 2.8903718 2.7080503 3.4011974 3.583519\n",
            " 2.7080503 2.4849067 2.0794415 1.609438  1.0986123      -inf      -inf]\n",
            "batch_rtg  [0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.6203638]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ScopeParamShapeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-f977faaa27b0>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    150\u001b[0m       \u001b[0mt_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;31m# Calculate how many timesteps we collected this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m       \u001b[0mA_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rtgs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV\u001b[0m \u001b[0;31m# ALG STEP 5 Calculate advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-f977faaa27b0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, batch_obs, batch_acts)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, variables, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m     \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_unbound_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m     return apply(\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m               flags=flags).temporary() as root:\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmutable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mscope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-46c2b1c9b25f>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[0;32m--> 187\u001b[0;31m     kernel = self.param('kernel',\n\u001b[0m\u001b[1;32m    188\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mparam\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m   1190\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNameInUseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'param'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'params'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mparam\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m           raise errors.ScopeParamShapeError(name, self.path_text,\n\u001b[0m\u001b[1;32m    811\u001b[0m                                             jnp.shape(val), jnp.shape(abs_val))\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: flax.errors.ScopeParamShapeError: Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/Dense_1\": (64, 800), (64, 1). (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-f977faaa27b0>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m       \u001b[0mt_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;31m# Calculate how many timesteps we collected this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m       \u001b[0mA_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rtgs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV\u001b[0m \u001b[0;31m# ALG STEP 5 Calculate advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0mA_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA_k\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mA_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Normalize advantages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-f977faaa27b0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, batch_obs, batch_acts)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m#V = self.critic(batch_obs).squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Calculate the log probabilities of batch actions using most\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-46c2b1c9b25f>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcritic_calculate_loss\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_rtgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[0;32m--> 187\u001b[0;31m     kernel = self.param('kernel',\n\u001b[0m\u001b[1;32m    188\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mScopeParamShapeError\u001b[0m: Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/Dense_1\": (64, 800), (64, 1). (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXP4lrq-wAjS"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}