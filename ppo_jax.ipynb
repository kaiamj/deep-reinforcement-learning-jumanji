{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQzF+i1vLWzGKUrb0kJTiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaiamj/deep-reinforcement-learning-jumanji/blob/main/ppo_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = jnp.array([])\n",
        "b2 = jnp.array([2])\n",
        "b = jnp.append(b,b2)"
      ],
      "metadata": {
        "id": "PGS90rmFxA_9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/instadeepai/jumanji.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NMWu06-soreA",
        "outputId": "32f32078-df62-4ba6-8fdf-e2e50154eefb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/instadeepai/jumanji.git\n",
            "  Cloning https://github.com/instadeepai/jumanji.git to /tmp/pip-req-build-39gxg0qs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/instadeepai/jumanji.git /tmp/pip-req-build-39gxg0qs\n",
            "  Resolved https://github.com/instadeepai/jumanji.git to commit 10958866909d434ba50edc1915247e4cebc3cb3e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting brax>=0.0.10\n",
            "  Downloading brax-0.1.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.3/471.3 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jax>=0.2.26 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.3.25)\n",
            "Requirement already satisfied: jaxlib>=0.1.74 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.3.25+cuda11.cudnn805)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (1.21.6)\n",
            "Collecting pygame>=2.0.0\n",
            "  Downloading pygame-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=9.0.0\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (4.4.0)\n",
            "Collecting chex>=0.1.3\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from jumanji==0.1.4) (0.25.2)\n",
            "Collecting matplotlib>=3.3.4\n",
            "  Downloading matplotlib-3.6.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-env>=1.5\n",
            "  Downloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.3.0)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.51.1)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (2.11.3)\n",
            "Collecting flax\n",
            "  Downloading flax-0.6.4-py3-none-any.whl (204 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.3/204.3 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco\n",
            "  Downloading mujoco-2.3.1.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from brax>=0.0.10->jumanji==0.1.4) (1.7.3)\n",
            "Collecting pytinyrenderer\n",
            "  Downloading pytinyrenderer-0.0.13-cp38-cp38-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optax\n",
            "  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trimesh==3.9.35\n",
            "  Downloading trimesh-3.9.35-py3-none-any.whl (639 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m639.3/639.3 KB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxopt\n",
            "  Downloading jaxopt-0.5.5-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from trimesh==3.9.35->brax>=0.0.10->jumanji==0.1.4) (57.4.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.3->jumanji==0.1.4) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.3->jumanji==0.1.4) (0.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (2.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.22.0->jumanji==0.1.4) (0.0.8)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.2.26->jumanji==0.1.4) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (3.0.9)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.3.4->jumanji==0.1.4) (1.4.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.22.0->jumanji==0.1.4) (3.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->jumanji==0.1.4) (1.15.0)\n",
            "Collecting tensorstore\n",
            "  Downloading tensorstore-0.1.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.4) (6.0)\n",
            "Collecting orbax\n",
            "  Downloading orbax-0.1.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich>=11.1\n",
            "  Downloading rich-13.2.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from flax->brax>=0.0.10->jumanji==0.1.4) (1.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->brax>=0.0.10->jumanji==0.1.4) (2.0.1)\n",
            "Collecting glfw\n",
            "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.8/dist-packages (from mujoco->brax>=0.0.10->jumanji==0.1.4) (3.1.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX->brax>=0.0.10->jumanji==0.1.4) (3.19.6)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.1->flax->brax>=0.0.10->jumanji==0.1.4) (2.6.1)\n",
            "Collecting markdown-it-py<3.0.0,>=2.1.0\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cached_property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.8/dist-packages (from orbax->flax->brax>=0.0.10->jumanji==0.1.4) (1.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.8/dist-packages (from orbax->flax->brax>=0.0.10->jumanji==0.1.4) (3.6.4)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.8/dist-packages (from orbax->flax->brax>=0.0.10->jumanji==0.1.4) (5.10.2)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (22.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (9.0.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest->orbax->flax->brax>=0.0.10->jumanji==0.1.4) (1.4.1)\n",
            "Building wheels for collected packages: jumanji\n",
            "  Building wheel for jumanji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jumanji: filename=jumanji-0.1.4-py3-none-any.whl size=166864 sha256=705fcdb3af39663d809e2a93f56acbed768b28a8a556729013faf5fd252b883d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hditexc7/wheels/df/c6/15/caef8b041b929f4f3b7a55a217c00f24c6931fe57ae40d9bd9\n",
            "Successfully built jumanji\n",
            "Installing collected packages: pytinyrenderer, glfw, dataclasses, cached_property, trimesh, tensorstore, tensorboardX, pygame, Pillow, mujoco, mdurl, fonttools, dm-env, contourpy, matplotlib, markdown-it-py, rich, jaxopt, chex, optax, orbax, flax, brax, jumanji\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed Pillow-9.4.0 brax-0.1.1 cached_property-1.5.2 chex-0.1.5 contourpy-1.0.7 dataclasses-0.6 dm-env-1.6 flax-0.6.4 fonttools-4.38.0 glfw-2.5.5 jaxopt-0.5.5 jumanji-0.1.4 markdown-it-py-2.1.0 matplotlib-3.6.3 mdurl-0.1.2 mujoco-2.3.1.post1 optax-0.1.4 orbax-0.1.0 pygame-2.1.2 pytinyrenderer-0.0.13 rich-13.2.0 tensorboardX-2.5.1 tensorstore-0.1.30 trimesh-3.9.35\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upuzSlMOHB8T",
        "outputId": "bc235d65-dfa9-4025-8200-74ba8d5144c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jumanji\n",
        "from jumanji.wrappers import AutoResetWrapper\n",
        "import flax.linen as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def flatten_jax(obs):\n",
        "    return jnp.concatenate([obs.ems.x1,obs.ems.x2,\n",
        "                        obs.ems.y1,obs.ems.y2,\n",
        "                        obs.ems.z1,obs.ems.z2,\n",
        "                        obs.ems_mask.flatten(),obs.items.x_len,\n",
        "                        obs.items.y_len,obs.items.z_len,\n",
        "                        obs.items_mask.flatten(),obs.items_placed.flatten()])"
      ],
      "metadata": {
        "id": "Ya00mTGWHGBJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn \n",
        "import optax\n",
        "\n",
        "class SimpleClassifierCompact(nn.Module):\n",
        "    num_hidden : int   # Number of hidden neurons\n",
        "    num_outputs : int  # Number of output neurons\n",
        "\n",
        "    @nn.compact  # Tells Flax to look for defined submodules\n",
        "    def __call__(self, x):\n",
        "        # Perform the calculation of the model to determine the prediction\n",
        "        # while defining necessary layers\n",
        "        x = nn.Dense(features=self.num_hidden)(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.Dense(features=self.num_outputs)(x)\n",
        "        return x\n",
        "def critic_calculate_loss( V,batch_rtgs):\n",
        "    #logits = critic_state.apply_fn(params, data).squeeze(axis=-1)\n",
        "    loss = optax.sigmoid_binary_cross_entropy(V, batch_rtgs).mean()\n",
        "    return loss\n",
        "\n",
        "@jax.jit  # Jit the function for efficiency\n",
        "def critic_train_step(state, V,batch_rtgs):\n",
        "    # Gradient function\n",
        "    grad_fn = jax.value_and_grad(critic_calculate_loss,  # Function to calculate the loss\n",
        "                                 argnums=1  # Parameters are second argument of the function\n",
        "                                 #has_aux=False  # Function has additional outputs, here accuracy\n",
        "                                )\n",
        "    # Determine gradients for current model, parameters and batch\n",
        "    loss, grads = grad_fn(V,batch_rtgs)\n",
        "    # Perform parameter update with gradients and optimizer\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    # Return state and any other value we might want\n",
        "    return state\n",
        "def actor_calculate_loss( surr1,surr2):\n",
        "    #logits = critic_state.apply_fn(params, data).squeeze(axis=-1)\n",
        "    actor_loss = (-jnp.min(surr1, surr2)).mean()\n",
        "    return actor_loss\n",
        "\n",
        "@jax.jit  # Jit the function for efficiency\n",
        "def actor_train_step(state,  surr1,surr2):\n",
        "    # Gradient function\n",
        "    grad_fn = jax.value_and_grad(actor_calculate_loss,  # Function to calculate the loss\n",
        "                                 argnums=1  # Parameters are second argument of the function\n",
        "                                 #has_aux=False  # Function has additional outputs, here accuracy\n",
        "                                )\n",
        "    # Determine gradients for current model, parameters and batch\n",
        "    loss, grads = grad_fn( surr1,surr2)\n",
        "    # Perform parameter update with gradients and optimizer\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    # Return state and any other value we might want\n",
        "    return state\n",
        "#critic_state, loss = critic_train_step(critic_state, data)"
      ],
      "metadata": {
        "id": "XOsEpJk7HMjx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training import train_state\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "class PPO:\n",
        "  def __init__(self,env):\n",
        "    self._init_hyperparameters()\n",
        "    self.env = env\n",
        "    \n",
        "    #initiate actor and critic\n",
        "    \n",
        "\n",
        "    self.optimizer = optax.sgd(learning_rate=self.lr)\n",
        "\n",
        "    self.actor = SimpleClassifierCompact(num_hidden=64, num_outputs=self.act_dim)\n",
        "    self.critic = SimpleClassifierCompact(num_hidden=64, num_outputs=1)\n",
        "    \n",
        "    self.params = self.actor.init(self.subkey, jnp.arange(self.obs_dim))\n",
        "\n",
        "    self.actor_state = train_state.TrainState.create(apply_fn=self.actor.apply,\n",
        "                                            params=self.params,\n",
        "                                            tx=self.optimizer)\n",
        "    self.critic_state = train_state.TrainState.create(apply_fn=self.critic.apply,\n",
        "                                            params=self.params,\n",
        "                                            tx=self.optimizer)\n",
        "     \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  def _init_hyperparameters(self):\n",
        "    # Default values for hyperparameters, will need to change later.\n",
        "    self.timesteps_per_batch = 2  #4800            # timesteps per batch\n",
        "    #self.max_timesteps_per_episode = 1600      # timesteps per episode\n",
        "    self.gamma = 0.95\n",
        "    self.n_updates_per_iteration = 1\n",
        "    self.clip = 0.2 # As recommended by the paper\n",
        "    self.lr = 0.1\n",
        "\n",
        "    self.obs_dim = 380\n",
        "    self.act_dim = 800\n",
        "\n",
        "    self.key = jax.random.PRNGKey(0)\n",
        "    self.key, self.subkey = jax.random.split(self.key)\n",
        "\n",
        "\n",
        "  def compute_rtgs_jax(self, batch_rews): \n",
        "    # The rewards-to-go (rtg) per episode per batch to return.\n",
        "    # The shape will be (num timesteps per episode)\n",
        "    \n",
        "    batch_rtgs = jnp.array([])\n",
        "    print(\"batch_rews \",batch_rews)\n",
        "    # Iterate through each episode backwards to maintain same order in batch_rtgs\n",
        "    for ep_rews in reversed(batch_rews):\n",
        "      discounted_reward = 0 # The discounted reward so far\n",
        "      print(\" ep_rews \",ep_rews)\n",
        "      discounted_reward = ep_rews + discounted_reward * self.gamma\n",
        "      print(\"discounted_reward \",discounted_reward)\n",
        "      batch_rtgs = jnp.insert(batch_rtgs, 0, discounted_reward)\n",
        "    print(\"batch_rtgs\",batch_rtgs)\n",
        "    return batch_rtgs\n",
        "\n",
        "\n",
        " \n",
        "  def get_action_jax(self, obs, action_jnp):\n",
        "    #flatten observation  p = flatten(timestep.observation)\n",
        "    # inside critic and actor by converting it to np.array(p) by actor you will get action\n",
        "\n",
        "    #initiate actor and critic\n",
        "     #actor = FeedForwardNN(380,800)\n",
        "     logits = self.actor_state.apply_fn(self.actor_state.params, obs)\n",
        "    \n",
        "     valid_indices = jnp.nonzero(action_jnp)  # getting valid indicies\n",
        "     valid_logits = logits[valid_indices]  # getting proper valid action probablities\n",
        "     valid_logits_array = jnp.reshape(valid_logits, (-1,))  # reshapping it to make it 1D\n",
        "     index_mapping = [i for i, include in enumerate(action_jnp) if include] # mapping of valid actions on whole set of actions\n",
        "     # Gumbel's trick\n",
        "     \n",
        "     u = jax.random.uniform(self.subkey, shape=valid_logits_array.shape) # generates random uniform values\n",
        "     \n",
        "     probs = valid_logits_array - jnp.log(-jnp.log(u)) # logits + random uniform noise\n",
        "     action = jnp.argmax(probs) # argmax of probs -> action id in filtered array from valid actions\n",
        "     action_id = index_mapping[action] # action index in the 800 size array from actor output\n",
        "     log_prob_action = jnp.log(action) # log probability of selected action\n",
        "     \n",
        "     return action_id, action, log_prob_action\n",
        "  \n",
        "  def rollout(self):\n",
        "    # batch observations, batch actions, log probs of each action, batch rewards,batch rewards-to-go,episodic lengths in batch\n",
        "    batch_obs, batch_acts, batch_log_probs, batch_rews, batch_rtgs, batch_lens,batch_states = jnp.array([]), jnp.array([]), jnp.array([]), jnp.array([]), jnp.array([]), jnp.array([]),jnp.array([])\n",
        "    \n",
        "    step_fn = jax.jit(self.env.step)\n",
        "    reset_fn = jax.jit(self.env.reset)\n",
        "    t = 0 \n",
        "    \n",
        "    while t < self.timesteps_per_batch: # Number of timesteps run so far this batch\n",
        "      # Rewards this episode\n",
        "      ep_rews = jnp.array([])\n",
        "      \n",
        "      \n",
        "      state, timestep = reset_fn(self.key)\n",
        "      ep_t = 1\n",
        "      rew = 0.0\n",
        "      while rew == 0.0:\n",
        "        ep_t += 1 # Increment timesteps ran this batch so far\n",
        "        obs = flatten_jax(timestep.observation)  # Collect observation\n",
        "        batch_obs = jnp.append(batch_obs, obs)\n",
        "        \n",
        "        num_ems, num_items = self.env.action_spec().num_values\n",
        "        action_mask = timestep.observation.action_mask.flatten()\n",
        "        action_jnp = jnp.array(action_mask, dtype=jnp.float32)\n",
        "\n",
        "        ems_item_id, action_,log_prob  = self.get_action_jax(obs,action_jnp)\n",
        "        ems_id, item_id = jnp.divmod(ems_item_id, num_items)\n",
        "\n",
        "        action = jnp.array([ems_id, item_id])  # Wrap the action as a jax array of shape (2,)\n",
        "        #batch_states = jnp.append(batch_states, state)\n",
        "\n",
        "        state,timestep = step_fn(state, action)\n",
        "        rew = jnp.array(timestep.reward.flatten())[0]\n",
        "        print(\" rew \", rew,\" type \", type(rew))\n",
        "        ep_rews = jnp.append(ep_rews, rew)\n",
        "        print(\" ep_rews \",ep_rews )\n",
        "        batch_acts = jnp.append(batch_acts, action_)\n",
        "        batch_log_probs = jnp.append(batch_log_probs, log_prob)\n",
        "      t += ep_t\n",
        "      batch_rews = jnp.append(batch_rews, ep_rews) \n",
        "    \n",
        "    batch_rtgs = self.compute_rtgs_jax(batch_rews)\n",
        "    return batch_obs, batch_acts,batch_log_probs, batch_rtgs, t ,rew\n",
        "\n",
        "  def learn(self, total_timesteps):\n",
        "    t_so_far = 0 # Timesteps simulated so far\n",
        "    episode_reward = jnp.array([])\n",
        "    while t_so_far < total_timesteps:              # ALG STEP \n",
        "      batch_obs, batch_acts, batch_log_probs, batch_rtgs, t, rew = self.rollout()\n",
        "      episode_reward = jnp.append(episode_reward, rew)\n",
        "      t_so_far += t # Calculate how many timesteps we collected this batch   \n",
        "      V, _ = self.evaluate(batch_obs, batch_acts)\n",
        "      A_k = batch_rtgs - V # ALG STEP 5 Calculate advantage\n",
        "      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) # Normalize advantages\n",
        "      for i in range(self.n_updates_per_iteration):\n",
        "        V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
        "        ratios = jax.lax.exp(curr_log_probs - batch_log_probs)   # Calculate ratios\n",
        "        surr1 = ratios * A_k  # Calculate surrogate losses\n",
        "        surr2 = jax.lax.clamp( 1 - self.clip, ratios, 1 + self.clip) * A_k\n",
        "        self.actor_state = actor_train_step(self.actor_state, surr1, surr2)\n",
        "        \n",
        "        self.critic_state = critic_train_step(self.critic_state, V, batch_rtgs)\n",
        "        \n",
        "    return episode_reward\n",
        "    \n",
        "  def evaluate(self, batch_obs,batch_acts):\n",
        "    # Query critic network for a value V for each obs in batch_obs.\n",
        "    #V = self.critic(batch_obs).squeeze()\n",
        "\n",
        "    V = self.critic_state.apply_fn(self.critic_state.params, batch_obs).squeeze(axis=-1)\n",
        "   \n",
        "    # Calculate the log probabilities of batch actions using most \n",
        "    # recent actor network.  # This segment of code is similar to that in get_action()\n",
        "    batch_logits = self.actor_state.apply_fn(self.actor_state.params, batch_obs).squeeze(axis=-1)\n",
        "    # rescaling \n",
        "    log_softmax_probs = jax.nn.log_softmax(batch_logits)\n",
        "    log_probs = log_softmax_probs[batch_acts]\n",
        "\n",
        "    return V, log_probs  # Return predicted values V and log probs log_probs\n",
        " \n",
        "  "
      ],
      "metadata": {
        "id": "UjzPa8LFHNvL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env = jumanji.make(\"BinPack-toy-v0\")\n",
        "model = PPO(env)\n",
        "#batch_obs, batch_acts, batch_log_probs, batch_rtgs, t, rew = model.rollout()\n",
        "  \n",
        "%time batch_states, episode_rewards = model.learn(1)\n",
        "#### with jit env , reset one episode \n",
        "# CPU times: user 7.4 s, sys: 103 ms, total: 7.5 s\n",
        "# Wall time: 7.1 s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x-37ugMco8ka",
        "outputId": "f4caa630-ac45-4101-d937-1b8a203f2ade"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n",
            "/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:173: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  return asarray(x, dtype=self.dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " rew  0.0  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " rew  0.6203638  type  <class 'jaxlib.xla_extension.DeviceArray'>\n",
            " ep_rews  [0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.6203638]\n",
            "batch_rews  [0.        0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.6203638]\n",
            " ep_rews  0.6203638\n",
            "discounted_reward  0.71536374\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            " ep_rews  0.0\n",
            "discounted_reward  0.095\n",
            "batch_rtgs [0.095      0.095      0.095      0.095      0.095      0.095\n",
            " 0.095      0.095      0.095      0.095      0.095      0.095\n",
            " 0.095      0.71536374]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ScopeParamShapeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-bcb7c0b65325>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    139\u001b[0m       \u001b[0mt_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;31m# Calculate how many timesteps we collected this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m       \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m       \u001b[0mA_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rtgs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV\u001b[0m \u001b[0;31m# ALG STEP 5 Calculate advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-bcb7c0b65325>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, batch_obs, batch_acts)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, variables, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m     \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_unbound_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m     return apply(\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m               flags=flags).temporary() as root:\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmutable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mscope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-03cc91638333>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# while defining necessary layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_derive_profiling_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[0;32m--> 187\u001b[0;31m     kernel = self.param('kernel',\n\u001b[0m\u001b[1;32m    188\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mparam\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m   1190\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNameInUseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'param'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'params'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mparam\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m           raise errors.ScopeParamShapeError(name, self.path_text,\n\u001b[0m\u001b[1;32m    811\u001b[0m                                             jnp.shape(val), jnp.shape(abs_val))\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: flax.errors.ScopeParamShapeError: Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/Dense_0\": (380, 64), (5320, 64). (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-bcb7c0b65325>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    138\u001b[0m       \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       \u001b[0mt_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;31m# Calculate how many timesteps we collected this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m       \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m       \u001b[0mA_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rtgs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV\u001b[0m \u001b[0;31m# ALG STEP 5 Calculate advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m       \u001b[0mA_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA_k\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mA_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Normalize advantages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-bcb7c0b65325>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, batch_obs, batch_acts)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m#V = self.critic(batch_obs).squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Calculate the log probabilities of batch actions using most\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-03cc91638333>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Perform the calculation of the model to determine the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# while defining necessary layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[0;32m--> 187\u001b[0;31m     kernel = self.param('kernel',\n\u001b[0m\u001b[1;32m    188\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mScopeParamShapeError\u001b[0m: Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/Dense_0\": (380, 64), (5320, 64). (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXP4lrq-wAjS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}